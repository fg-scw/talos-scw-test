#cloud-config

package_update: true
package_upgrade: false

packages:
  - curl
  - wget
  - jq
  - netcat-openbsd

write_files:
  - path: /root/cilium-patch.yaml
    permissions: '0644'
    content: |
      cluster:
        proxy:
          disabled: true
        network:
          cni:
            name: none

  - path: /root/gpu-worker-patch.yaml
    permissions: '0644'
    content: |
      machine:
        kernel:
          modules:
            - name: nvidia
            - name: nvidia_uvm
            - name: nvidia_drm
            - name: nvidia_modeset
        sysctls:
          net.core.bpf_jit_harden: "1"

  - path: /root/nvidia-runtime-class.yaml
    permissions: '0644'
    content: |
      apiVersion: node.k8s.io/v1
      kind: RuntimeClass
      metadata:
        name: nvidia
      handler: nvidia

  # ==========================================================================
  # MIG Configuration Script (standalone file - no template escaping issues)
  # ==========================================================================
  - path: /root/configure-mig.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      
      MIG_PROFILE="$1"
      
      log() { echo "[MIG-CONFIG] $(date '+%Y-%m-%d %H:%M:%S') $*"; }
      
      # Check if MIG is enabled in config
      if [ -z "$MIG_PROFILE" ] || [ "$MIG_PROFILE" = "disabled" ] || [ "$MIG_PROFILE" = "all-disabled" ]; then
        log "MIG disabled by configuration, skipping"
        exit 0
      fi
      
      # Check if GPU supports MIG
      GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -1)
      log "Detected GPU: $GPU_NAME"
      
      if ! echo "$GPU_NAME" | grep -qE "(H100|A100|A30)"; then
        log "GPU does not support MIG, skipping"
        exit 0
      fi
      
      get_mig_params() {
        case $1 in
          "all-1g.10gb")  echo "19,19,19,19,19,19,19" ;;
          "all-2g.20gb")  echo "14,14,14" ;;
          "all-3g.40gb")  echo "9,9" ;;
          "mixed-40-20")  echo "9,14,14" ;;
          *)              echo "" ;;
        esac
      }
      
      get_expected_count() {
        case $1 in
          "all-1g.10gb")  echo "7" ;;
          "all-2g.20gb")  echo "3" ;;
          "all-3g.40gb")  echo "2" ;;
          "mixed-40-20")  echo "3" ;;
          *)              echo "0" ;;
        esac
      }
      
      MIG_PARAMS=$(get_mig_params "$MIG_PROFILE")
      EXPECTED_COUNT=$(get_expected_count "$MIG_PROFILE")
      
      if [ -z "$MIG_PARAMS" ]; then
        log "Unknown MIG profile: $MIG_PROFILE"
        exit 1
      fi
      
      log "Target MIG profile: $MIG_PROFILE ($EXPECTED_COUNT instances)"
      
      # Check MIG mode
      MIG_MODE=$(nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | tr -d ' ')
      log "Current MIG mode: $MIG_MODE"
      
      if [ "$MIG_MODE" != "Enabled" ]; then
        log "ERROR: MIG mode not enabled. Node reboot required after enabling."
        exit 1
      fi
      
      # Check existing instances
      EXISTING=$(nvidia-smi mig -lgi 2>/dev/null | grep -c "MIG" || echo "0")
      log "Existing MIG instances: $EXISTING"
      
      if [ "$EXISTING" -ge "$EXPECTED_COUNT" ]; then
        log "MIG already configured ($EXISTING instances)"
        nvidia-smi mig -lgi
        exit 0
      fi
      
      # Destroy and recreate
      log "Reconfiguring MIG instances..."
      nvidia-smi mig -dci 2>/dev/null || true
      nvidia-smi mig -dgi 2>/dev/null || true
      sleep 2
      
      log "Creating instances: $MIG_PARAMS"
      nvidia-smi mig -cgi "$MIG_PARAMS" -C
      
      log "MIG configuration complete:"
      nvidia-smi mig -lgi

  # ==========================================================================
  # NVIDIA Device Plugin DaemonSet with MIG init container
  # ==========================================================================
  - path: /root/nvidia-device-plugin.yaml
    permissions: '0644'
    content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: nvidia-device-plugin
        labels:
          pod-security.kubernetes.io/enforce: privileged
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mig-config
        namespace: nvidia-device-plugin
      data:
        MIG_PROFILE: "${gpu_mig_profile}"
        configure-mig.sh: |
          #!/bin/bash
          set -e
          MIG_PROFILE="${gpu_mig_profile}"
          ENABLE_MIG="${enable_gpu_mig}"
          
          log() { echo "[MIG-CONFIG] $*"; }
          
          if [ "$ENABLE_MIG" != "true" ]; then
            log "MIG disabled"; exit 0
          fi
          
          GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -1)
          log "GPU: $GPU_NAME"
          
          if ! echo "$GPU_NAME" | grep -qE "(H100|A100|A30)"; then
            log "GPU does not support MIG"; exit 0
          fi
          
          if [ "$MIG_PROFILE" = "disabled" ] || [ "$MIG_PROFILE" = "all-disabled" ]; then
            log "MIG profile disabled"; exit 0
          fi
          
          case $MIG_PROFILE in
            "all-1g.10gb") MIG_PARAMS="19,19,19,19,19,19,19"; EXPECTED=7 ;;
            "all-2g.20gb") MIG_PARAMS="14,14,14"; EXPECTED=3 ;;
            "all-3g.40gb") MIG_PARAMS="9,9"; EXPECTED=2 ;;
            "mixed-40-20") MIG_PARAMS="9,14,14"; EXPECTED=3 ;;
            *) log "Unknown profile"; exit 1 ;;
          esac
          
          log "Profile: $MIG_PROFILE ($EXPECTED instances)"
          
          MIG_MODE=$(nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | tr -d ' ')
          log "MIG mode: $MIG_MODE"
          
          if [ "$MIG_MODE" != "Enabled" ]; then
            log "MIG not enabled - skipping instance creation"; exit 0
          fi
          
          EXISTING=$(nvidia-smi mig -lgi 2>/dev/null | grep -c "MIG" || echo "0")
          log "Existing instances: $EXISTING"
          
          if [ "$EXISTING" -ge "$EXPECTED" ]; then
            log "Already configured"; exit 0
          fi
          
          log "Creating MIG instances..."
          nvidia-smi mig -dci 2>/dev/null || true
          nvidia-smi mig -dgi 2>/dev/null || true
          sleep 2
          nvidia-smi mig -cgi "$MIG_PARAMS" -C
          log "Done"
          nvidia-smi mig -lgi
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nvidia-device-plugin-daemonset
        namespace: nvidia-device-plugin
      spec:
        selector:
          matchLabels:
            name: nvidia-device-plugin-ds
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              name: nvidia-device-plugin-ds
          spec:
            priorityClassName: system-node-critical
            runtimeClassName: nvidia
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            nodeSelector:
              nvidia.com/gpu.present: "true"
            initContainers:
              - name: mig-configurator
                image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
                command: ["/bin/bash", "/scripts/configure-mig.sh"]
                securityContext:
                  privileged: true
                volumeMounts:
                  - name: mig-scripts
                    mountPath: /scripts
            containers:
              - name: nvidia-device-plugin-ctr
                image: nvcr.io/nvidia/k8s-device-plugin:v0.14.5
                env:
                  - name: FAIL_ON_INIT_ERROR
                    value: "false"
                  - name: MIG_STRATEGY
                    value: "single"
                securityContext:
                  privileged: true
                volumeMounts:
                  - name: device-plugin
                    mountPath: /var/lib/kubelet/device-plugins
            volumes:
              - name: device-plugin
                hostPath:
                  path: /var/lib/kubelet/device-plugins
              - name: mig-scripts
                configMap:
                  name: mig-config
                  defaultMode: 0755

  # ==========================================================================
  # MIG Enable Pod (for initial MIG mode activation)
  # ==========================================================================
  - path: /root/mig-enable-pod.yaml
    permissions: '0644'
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: mig-enable
        namespace: nvidia-device-plugin
      spec:
        restartPolicy: Never
        runtimeClassName: nvidia
        nodeSelector:
          nvidia.com/gpu.present: "true"
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        hostPID: true
        containers:
          - name: mig-enable
            image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
            command:
              - /bin/bash
              - -c
              - |
                echo "Checking MIG mode..."
                nvidia-smi --query-gpu=mig.mode.current,mig.mode.pending --format=csv
                
                MIG_CURRENT=$(nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | tr -d ' ')
                
                if [ "$MIG_CURRENT" = "Enabled" ]; then
                  echo "MIG already enabled!"
                  exit 0
                fi
                
                echo "Enabling MIG mode..."
                nvidia-smi -i 0 -mig 1
                
                MIG_PENDING=$(nvidia-smi --query-gpu=mig.mode.pending --format=csv,noheader | tr -d ' ')
                echo "MIG pending: $MIG_PENDING"
                
                if [ "$MIG_PENDING" = "Enabled" ]; then
                  echo "MIG enabled - reboot required"
                  exit 100
                fi
                
                # If current is now Enabled (no reboot needed)
                MIG_CURRENT=$(nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | tr -d ' ')
                if [ "$MIG_CURRENT" = "Enabled" ]; then
                  echo "MIG enabled immediately!"
                  exit 0
                fi
                
                exit 1
            securityContext:
              privileged: true

  - path: /root/bootstrap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail
      
      CLUSTER_NAME="${cluster_name}"
      K8S_ENDPOINT="https://${k8s_api_endpoint}:6443"
      CONTROL_PLANE_IPS=(${control_plane_ips})
      GPU_WORKER_IPS=(${gpu_worker_ips})
      CPU_WORKER_IPS=(${cpu_worker_ips})
      TOTAL_NODES=$((${control_plane_count} + ${gpu_worker_count} + ${cpu_worker_count}))
      CONFIG_DIR="/root/talos-config"
      TALOSCONFIG="$CONFIG_DIR/talosconfig"
      LOG_FILE="/var/log/talos-bootstrap.log"
      ENABLE_GPU_MIG="${enable_gpu_mig}"
      GPU_MIG_PROFILE="${gpu_mig_profile}"
      GPU_SCHEMATIC_ID="${gpu_schematic_id}"
      TALOS_VERSION="${talos_version}"
      
      ALL_WORKER_IPS=()
      [[ $${#GPU_WORKER_IPS[@]} -gt 0 ]] && ALL_WORKER_IPS+=("$${GPU_WORKER_IPS[@]}")
      [[ $${#CPU_WORKER_IPS[@]} -gt 0 ]] && ALL_WORKER_IPS+=("$${CPU_WORKER_IPS[@]}")
      
      log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"; }
      log_ok() { log "[OK] $*"; }
      log_warn() { log "[WARN] $*"; }
      log_error() { log "[ERROR] $*"; }
      
      wait_for_nodes_ready() {
        local target=$1 timeout=$2 count=0
        log "Waiting for $target nodes to be Ready..."
        while [ $count -lt $timeout ]; do
          local ready
          ready=$(kubectl get nodes --no-headers 2>/dev/null | grep -c " Ready " || true)
          ready=$${ready:-0}
          log "  Ready: $ready/$target"
          if [ "$ready" -ge "$target" ] 2>/dev/null; then
            log_ok "All $target nodes Ready"
            return 0
          fi
          sleep 10
          count=$((count + 10))
        done
        log_warn "Timeout waiting for nodes"
        return 1
      }
      
      check_nvidia_extensions() {
        local gpu_ip=$1
        talosctl --talosconfig "$TALOSCONFIG" -n "$gpu_ip" get extensions 2>/dev/null | grep -q "nvidia"
      }
      
      wait_for_nvidia_modules() {
        local gpu_ip=$1 timeout=300 count=0
        log "Waiting for NVIDIA modules on $gpu_ip..."
        while [ $count -lt $timeout ]; do
          if talosctl --talosconfig "$TALOSCONFIG" -n "$gpu_ip" read /proc/modules 2>/dev/null | grep -q "^nvidia "; then
            log_ok "NVIDIA modules loaded"
            return 0
          fi
          log "  Waiting... ($count/$timeout)"
          sleep 10
          count=$((count + 10))
        done
        return 1
      }
      
      wait_for_gpu_node_ready() {
        local gpu_ip=$1 timeout=$${2:-300} count=0
        log "Waiting for GPU node $gpu_ip to be Ready..."
        while [ $count -lt $timeout ]; do
          if kubectl get nodes -o wide --no-headers 2>/dev/null | grep "$gpu_ip" | grep -q " Ready "; then
            log_ok "GPU node Ready"
            return 0
          fi
          log "  Waiting... ($count/$timeout)"
          sleep 10
          count=$((count + 10))
        done
        log_warn "Timeout waiting for GPU node"
        return 1
      }
      
      enable_mig_and_configure() {
        local gpu_ip=$1
        local mig_profile=$2
        
        log "=========================================="
        log "MIG Configuration: $mig_profile"
        log "=========================================="
        
        # Create namespace
        kubectl create namespace nvidia-device-plugin --dry-run=client -o yaml | kubectl apply -f -
        kubectl label namespace nvidia-device-plugin pod-security.kubernetes.io/enforce=privileged --overwrite
        
        # Deploy MIG enable pod
        kubectl apply -f /root/mig-enable-pod.yaml
        
        log "Waiting for MIG enable pod..."
        local max_wait=60
        for i in $(seq 1 $max_wait); do
          STATUS=$(kubectl -n nvidia-device-plugin get pod mig-enable -o jsonpath='{.status.phase}' 2>/dev/null || echo "Pending")
          
          if [ "$STATUS" = "Succeeded" ]; then
            log_ok "MIG already enabled"
            kubectl delete pod -n nvidia-device-plugin mig-enable --ignore-not-found
            configure_mig_instances "$mig_profile"
            return 0
          elif [ "$STATUS" = "Failed" ]; then
            EXIT_CODE=$(kubectl -n nvidia-device-plugin get pod mig-enable -o jsonpath='{.status.containerStatuses[0].state.terminated.exitCode}' 2>/dev/null || echo "0")
            
            if [ "$EXIT_CODE" = "100" ]; then
              log "MIG mode enabled, reboot required"
              kubectl delete pod -n nvidia-device-plugin mig-enable --ignore-not-found
              
              log "Rebooting GPU node..."
              talosctl --talosconfig "$TALOSCONFIG" -n "$gpu_ip" reboot || true
              
              log "Waiting 120s for reboot..."
              sleep 120
              
              wait_for_gpu_node_ready "$gpu_ip" 300
              wait_for_nvidia_modules "$gpu_ip"
              
              configure_mig_instances "$mig_profile"
              return 0
            elif [ "$EXIT_CODE" = "0" ]; then
              log_ok "MIG enabled without reboot"
              kubectl delete pod -n nvidia-device-plugin mig-enable --ignore-not-found
              configure_mig_instances "$mig_profile"
              return 0
            else
              log_error "MIG enable failed with exit code $EXIT_CODE"
              kubectl logs -n nvidia-device-plugin mig-enable || true
              kubectl delete pod -n nvidia-device-plugin mig-enable --ignore-not-found
              return 1
            fi
          fi
          
          log "  Status: $STATUS ($i/$max_wait)"
          sleep 5
        done
        
        kubectl delete pod -n nvidia-device-plugin mig-enable --ignore-not-found
        log_warn "Timeout waiting for MIG enable"
        return 1
      }
      
      configure_mig_instances() {
        local mig_profile=$1
        
        log "Creating MIG instances for profile: $mig_profile"
        
        # Get MIG parameters
        case $mig_profile in
          "all-1g.10gb") MIG_PARAMS="19,19,19,19,19,19,19"; EXPECTED=7 ;;
          "all-2g.20gb") MIG_PARAMS="14,14,14"; EXPECTED=3 ;;
          "all-3g.40gb") MIG_PARAMS="9,9"; EXPECTED=2 ;;
          "mixed-40-20") MIG_PARAMS="9,14,14"; EXPECTED=3 ;;
          *) log_error "Unknown profile: $mig_profile"; return 1 ;;
        esac
        
        # Create temporary pod to configure MIG
        cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: Pod
      metadata:
        name: mig-configure
        namespace: nvidia-device-plugin
      spec:
        restartPolicy: Never
        runtimeClassName: nvidia
        nodeSelector:
          nvidia.com/gpu.present: "true"
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        containers:
          - name: mig-configure
            image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
            command:
              - /bin/bash
              - -c
              - |
                echo "Checking MIG mode..."
                MIG_MODE=\$(nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | tr -d ' ')
                echo "MIG mode: \$MIG_MODE"
                
                if [ "\$MIG_MODE" != "Enabled" ]; then
                  echo "ERROR: MIG not enabled"
                  exit 1
                fi
                
                echo "Destroying existing instances..."
                nvidia-smi mig -dci 2>/dev/null || true
                nvidia-smi mig -dgi 2>/dev/null || true
                sleep 2
                
                echo "Creating MIG instances: $MIG_PARAMS"
                nvidia-smi mig -cgi $MIG_PARAMS -C
                
                echo "Result:"
                nvidia-smi mig -lgi
            securityContext:
              privileged: true
      EOF
        
        log "Waiting for MIG configure pod..."
        for i in $(seq 1 30); do
          STATUS=$(kubectl -n nvidia-device-plugin get pod mig-configure -o jsonpath='{.status.phase}' 2>/dev/null || echo "Pending")
          if [ "$STATUS" = "Succeeded" ]; then
            log_ok "MIG instances created"
            kubectl logs -n nvidia-device-plugin mig-configure || true
            kubectl delete pod -n nvidia-device-plugin mig-configure --ignore-not-found
            return 0
          elif [ "$STATUS" = "Failed" ]; then
            log_error "MIG configure failed"
            kubectl logs -n nvidia-device-plugin mig-configure || true
            kubectl delete pod -n nvidia-device-plugin mig-configure --ignore-not-found
            return 1
          fi
          log "  Status: $STATUS ($i/30)"
          sleep 10
        done
        
        kubectl delete pod -n nvidia-device-plugin mig-configure --ignore-not-found
        return 1
      }
      
      # =======================================================================
      # MAIN BOOTSTRAP
      # =======================================================================
      
      log "=========================================="
      log "Talos Cluster Bootstrap - $CLUSTER_NAME"
      log "=========================================="
      log "Endpoint: $K8S_ENDPOINT"
      log "Control Planes: $${CONTROL_PLANE_IPS[*]}"
      log "GPU Workers: $${GPU_WORKER_IPS[*]:-none}"
      log "CPU Workers: $${CPU_WORKER_IPS[*]:-none}"
      log "Total nodes: $TOTAL_NODES"
      log "GPU MIG: $ENABLE_GPU_MIG (profile: $GPU_MIG_PROFILE)"
      
      # Wait for all nodes
      log "Waiting for nodes to be reachable..."
      for ip in "$${CONTROL_PLANE_IPS[@]}" "$${ALL_WORKER_IPS[@]}"; do
        log "  Checking $ip:50000..."
        until nc -z -w5 "$ip" 50000 2>/dev/null; do sleep 5; done
        log_ok "$ip reachable"
      done
      
      # Generate Talos configs
      log "Generating Talos configurations..."
      mkdir -p "$CONFIG_DIR"
      talosctl gen config "$CLUSTER_NAME" "$K8S_ENDPOINT" \
        --output-dir "$CONFIG_DIR" \
        --with-docs=false \
        --with-examples=false
      
      talosctl --talosconfig "$TALOSCONFIG" config endpoint $${CONTROL_PLANE_IPS[@]}
      talosctl --talosconfig "$TALOSCONFIG" config node $${CONTROL_PLANE_IPS[0]}
      
      # Patch configs
      log "Creating patched configurations..."
      talosctl machineconfig patch "$CONFIG_DIR/controlplane.yaml" \
        --patch @/root/cilium-patch.yaml \
        -o "$CONFIG_DIR/controlplane-patched.yaml"
      
      talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
        --patch @/root/cilium-patch.yaml \
        -o "$CONFIG_DIR/worker-cpu-patched.yaml"
      
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
          --patch @/root/cilium-patch.yaml \
          --patch @/root/gpu-worker-patch.yaml \
          -o "$CONFIG_DIR/worker-gpu-patched.yaml"
      fi
      
      # Apply configs
      log "Applying configurations..."
      for ip in "$${CONTROL_PLANE_IPS[@]}"; do
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/controlplane-patched.yaml"
        log_ok "Configured $ip (control-plane)"
      done
      
      for ip in "$${GPU_WORKER_IPS[@]}"; do
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-gpu-patched.yaml"
        log_ok "Configured $ip (GPU worker)"
      done
      
      for ip in "$${CPU_WORKER_IPS[@]}"; do
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-cpu-patched.yaml"
        log_ok "Configured $ip (CPU worker)"
      done
      
      log "Waiting 120s for configuration..."
      sleep 120
      
      # Bootstrap etcd
      log "Bootstrapping etcd on $${CONTROL_PLANE_IPS[0]}..."
      talosctl --talosconfig "$TALOSCONFIG" bootstrap --nodes "$${CONTROL_PLANE_IPS[0]}"
      log_ok "etcd bootstrap initiated"
      
      log "Waiting 90s for cluster..."
      sleep 90
      
      # Get kubeconfig
      log "Fetching kubeconfig..."
      talosctl --talosconfig "$TALOSCONFIG" kubeconfig "$CONFIG_DIR/kubeconfig" \
        --nodes "$${CONTROL_PLANE_IPS[0]}" --force
      export KUBECONFIG="$CONFIG_DIR/kubeconfig"
      
      log "Waiting for Kubernetes API..."
      for i in {1..60}; do
        kubectl get nodes &>/dev/null && { log_ok "API ready"; break; }
        sleep 10
      done
      
      kubectl get nodes -o wide || true
      
      # Install Cilium
      log "Installing Cilium CNI..."
      helm repo add cilium https://helm.cilium.io/ 2>/dev/null || true
      helm repo update
      
      helm install cilium cilium/cilium \
        --namespace kube-system \
        --set ipam.mode=kubernetes \
        --set kubeProxyReplacement=true \
        --set k8sServiceHost=${k8s_api_endpoint} \
        --set k8sServicePort=6443 \
        --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
        --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
        --set cgroup.autoMount.enabled=true \
        --set cgroup.hostRoot=/sys/fs/cgroup \
        --timeout 5m || log_warn "Cilium install issue"
      
      log_ok "Cilium installed"
      wait_for_nodes_ready $TOTAL_NODES 600 || true
      
      # GPU Setup
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        log "=========================================="
        log "Setting up GPU Stack"
        log "=========================================="
        
        kubectl label namespace kube-system pod-security.kubernetes.io/enforce=privileged --overwrite
        kubectl apply -f /root/nvidia-runtime-class.yaml
        log_ok "RuntimeClass created"
        
        GPU_IP="$${GPU_WORKER_IPS[0]}"
        
        # Verify NVIDIA extensions
        log "Checking NVIDIA extensions on $GPU_IP..."
        if check_nvidia_extensions "$GPU_IP"; then
          log_ok "NVIDIA extensions present"
        else
          log_warn "NVIDIA extensions check failed (may still work)"
        fi
        
        wait_for_nvidia_modules "$GPU_IP" || log_warn "NVIDIA modules may not be ready"
        
        # Label GPU nodes
        log "Labeling GPU nodes..."
        sleep 30
        for ip in "$${GPU_WORKER_IPS[@]}"; do
          NODE_NAME=$(kubectl get nodes -o wide --no-headers | grep "$ip" | awk '{print $1}' || true)
          if [ -n "$NODE_NAME" ]; then
            kubectl label node "$NODE_NAME" nvidia.com/gpu.present=true --overwrite || true
            log_ok "Labeled $NODE_NAME"
          fi
        done
        
        # Install NFD
        log "Installing Node Feature Discovery..."
        helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts 2>/dev/null || true
        helm repo update
        helm install node-feature-discovery nfd/node-feature-discovery \
          --namespace node-feature-discovery \
          --create-namespace \
          --timeout 5m || log_warn "NFD issue"
        kubectl label namespace node-feature-discovery pod-security.kubernetes.io/enforce=privileged --overwrite
        log_ok "NFD installed"
        
        sleep 30
        
        # Enable MIG and configure instances
        if [ "$ENABLE_GPU_MIG" = "true" ] && [ "$GPU_MIG_PROFILE" != "all-disabled" ]; then
          enable_mig_and_configure "$GPU_IP" "$GPU_MIG_PROFILE"
        fi
        
        # Deploy Device Plugin
        log "Installing NVIDIA Device Plugin..."
        kubectl apply -f /root/nvidia-device-plugin.yaml
        log_ok "Device Plugin deployed"
        
        # Wait for device plugin
        log "Waiting for device plugin..."
        for i in {1..60}; do
          kubectl -n nvidia-device-plugin get pods -l name=nvidia-device-plugin-ds --no-headers 2>/dev/null | grep -q Running && break
          log "  Waiting... ($i/60)"
          sleep 10
        done
        
        # Check GPU resources
        EXPECTED_GPUS=1
        [ "$ENABLE_GPU_MIG" = "true" ] && case "$GPU_MIG_PROFILE" in
          "all-1g.10gb") EXPECTED_GPUS=7 ;;
          "all-2g.20gb") EXPECTED_GPUS=3 ;;
          "all-3g.40gb") EXPECTED_GPUS=2 ;;
          "mixed-40-20") EXPECTED_GPUS=3 ;;
        esac
        
        log "Waiting for GPU resources (expecting $EXPECTED_GPUS)..."
        for i in {1..30}; do
          GPU_COUNT=$(kubectl get nodes -o json 2>/dev/null | jq '[.items[].status.allocatable["nvidia.com/gpu"] // "0" | tonumber] | add' || echo "0")
          GPU_COUNT=$${GPU_COUNT:-0}
          if [ "$GPU_COUNT" -ge "$EXPECTED_GPUS" ] 2>/dev/null; then
            log_ok "Found $GPU_COUNT GPU(s)"
            break
          fi
          log "  GPUs: $GPU_COUNT/$EXPECTED_GPUS ($i/30)"
          sleep 10
        done
      fi
      
      log "=========================================="
      log "Bootstrap Complete!"
      log "=========================================="
      kubectl get nodes -o wide
      
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        log "GPU Resources:"
        kubectl get nodes -o custom-columns='NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu'
      fi
      
      log "Configs: $CONFIG_DIR"
      log "Usage: export KUBECONFIG=$CONFIG_DIR/kubeconfig"

  - path: /etc/systemd/system/talos-bootstrap.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Talos Cluster Bootstrap
      After=network-online.target
      Wants=network-online.target
      
      [Service]
      Type=oneshot
      User=root
      ExecStart=/root/bootstrap.sh
      StandardOutput=journal+console
      StandardError=journal+console
      RemainAfterExit=yes
      
      [Install]
      WantedBy=multi-user.target

runcmd:
  - curl -sL https://talos.dev/install | sh
  - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && chmod +x kubectl && mv kubectl /usr/local/bin/
  - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  - systemctl daemon-reload
  - systemctl enable --now talos-bootstrap.service
