#cloud-config
# =============================================================================
# Talos Cluster Bootstrap - PRODUCTION READY
# =============================================================================
# CRITICAL FIXES INCLUDED:
#   - Device plugin v0.14.5 (NOT v0.17.0 - has Talos CDI/glibc issues)
#   - DEVICE_LIST_STRATEGY=envvar (NOT cdi-annotations)
#   - MIG pods run in nvidia-gpu-stack namespace (privileged allowed)
# =============================================================================

package_update: true
package_upgrade: false

packages:
  - curl
  - wget
  - netcat-openbsd
  - jq

write_files:
  # ===========================================================================
  # Talos Configuration Patches
  # ===========================================================================

  - path: /root/patches/cilium.yaml
    permissions: '0644'
    content: |
      cluster:
        proxy:
          disabled: true
        network:
          cni:
            name: none

  - path: /root/patches/gpu-worker.yaml
    permissions: '0644'
    content: |
      machine:
        kernel:
          modules:
            - name: nvidia
            - name: nvidia_uvm
            - name: nvidia_drm
            - name: nvidia_modeset
        sysctls:
          net.core.bpf_jit_harden: "1"

  # ===========================================================================
  # GPU Stack Base (WITHOUT device plugin - deployed first)
  # ===========================================================================

  - path: /root/manifests/gpu-stack-base.yaml
    permissions: '0644'
    content: |
      ---
      apiVersion: v1
      kind: Namespace
      metadata:
        name: nvidia-gpu-stack
        labels:
          pod-security.kubernetes.io/enforce: privileged
          pod-security.kubernetes.io/audit: privileged
          pod-security.kubernetes.io/warn: privileged
      ---
      apiVersion: node.k8s.io/v1
      kind: RuntimeClass
      metadata:
        name: nvidia
      handler: nvidia
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mig-config
        namespace: nvidia-gpu-stack
      data:
        MIG_ENABLED: "${mig_enabled}"
        MIG_PROFILE: "${mig_profile}"
        MIG_PROFILE_CONFIG: "${mig_profile_config}"
        MIG_INSTANCE_COUNT: "${mig_instance_count}"
      ---
      apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      metadata:
        name: nodefeatures.nfd.k8s-sigs.io
      spec:
        group: nfd.k8s-sigs.io
        names:
          kind: NodeFeature
          listKind: NodeFeatureList
          plural: nodefeatures
          singular: nodefeature
        scope: Namespaced
        versions:
          - name: v1alpha1
            served: true
            storage: true
            schema:
              openAPIV3Schema:
                type: object
                properties:
                  spec:
                    type: object
                    x-kubernetes-preserve-unknown-fields: true
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: nfd-master
        namespace: nvidia-gpu-stack
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: nfd-worker
        namespace: nvidia-gpu-stack
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: gpu-operator
        namespace: nvidia-gpu-stack
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: nfd-master
      rules:
        - apiGroups: [""]
          resources: ["nodes", "nodes/status"]
          verbs: ["get", "list", "watch", "patch", "update"]
        - apiGroups: ["nfd.k8s-sigs.io"]
          resources: ["nodefeatures"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["coordination.k8s.io"]
          resources: ["leases"]
          verbs: ["create", "get", "update"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: nfd-master
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: nfd-master
      subjects:
        - kind: ServiceAccount
          name: nfd-master
          namespace: nvidia-gpu-stack
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: nfd-worker
      rules:
        - apiGroups: ["nfd.k8s-sigs.io"]
          resources: ["nodefeatures"]
          verbs: ["create", "get", "update"]
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "watch"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: nfd-worker
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: nfd-worker
      subjects:
        - kind: ServiceAccount
          name: nfd-worker
          namespace: nvidia-gpu-stack
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: gpu-operator
      rules:
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "watch", "patch", "update"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: gpu-operator
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: gpu-operator
      subjects:
        - kind: ServiceAccount
          name: gpu-operator
          namespace: nvidia-gpu-stack
      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: nfd-master
        namespace: nvidia-gpu-stack
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: nfd-master
        template:
          metadata:
            labels:
              app: nfd-master
          spec:
            serviceAccountName: nfd-master
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
            containers:
              - name: nfd-master
                image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
                command: ["nfd-master"]
                args: ["-port=8080"]
                ports:
                  - containerPort: 8080
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: nfd-master
        namespace: nvidia-gpu-stack
      spec:
        selector:
          app: nfd-master
        ports:
          - port: 8080
            name: grpc
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nfd-worker
        namespace: nvidia-gpu-stack
      spec:
        selector:
          matchLabels:
            app: nfd-worker
        template:
          metadata:
            labels:
              app: nfd-worker
          spec:
            serviceAccountName: nfd-worker
            dnsPolicy: ClusterFirstWithHostNet
            tolerations:
              - operator: Exists
            containers:
              - name: nfd-worker
                image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
                command: ["nfd-worker"]
                args: ["-server=nfd-master.nvidia-gpu-stack.svc.cluster.local:8080"]
                env:
                  - name: NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                volumeMounts:
                  - name: host-boot
                    mountPath: /host-boot
                    readOnly: true
                  - name: host-sys
                    mountPath: /host-sys
                    readOnly: true
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
            volumes:
              - name: host-boot
                hostPath:
                  path: /boot
              - name: host-sys
                hostPath:
                  path: /sys
      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: gpu-labeler
        namespace: nvidia-gpu-stack
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: gpu-labeler
        template:
          metadata:
            labels:
              app: gpu-labeler
          spec:
            serviceAccountName: gpu-operator
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
            containers:
              - name: labeler
                image: bitnami/kubectl:latest
                command: ["/bin/bash", "-c"]
                args:
                  - |
                    while true; do
                      for node in $(kubectl get nodes -l feature.node.kubernetes.io/pci-10de.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
                        kubectl label node $node nvidia.com/gpu.present=true --overwrite 2>/dev/null || true
                      done
                      sleep 30
                    done
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 50m
                    memory: 64Mi

  # ===========================================================================
  # Device Plugin (deployed AFTER MIG configuration)
  # ===========================================================================

  - path: /root/manifests/device-plugin.yaml
    permissions: '0644'
    content: |
      # =========================================================================
      # NVIDIA Device Plugin - v0.14.5 (CRITICAL)
      # =========================================================================
      # MUST be deployed AFTER MIG instances are created!
      # v0.14.5 + envvar strategy = WORKS on Talos
      # DO NOT USE v0.17.0 - has Talos CDI/glibc issues
      # =========================================================================
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nvidia-device-plugin
        namespace: nvidia-gpu-stack
      spec:
        selector:
          matchLabels:
            app: nvidia-device-plugin
        template:
          metadata:
            labels:
              app: nvidia-device-plugin
          spec:
            runtimeClassName: nvidia
            nodeSelector:
              nvidia.com/gpu.present: "true"
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            priorityClassName: system-node-critical
            containers:
              - name: nvidia-device-plugin
                image: nvcr.io/nvidia/k8s-device-plugin:v0.14.5
                securityContext:
                  privileged: true
                env:
                  - name: DEVICE_LIST_STRATEGY
                    value: "envvar"
                  - name: PASS_DEVICE_SPECS
                    value: "true"
                  - name: FAIL_ON_INIT_ERROR
                    value: "false"
                  - name: MIG_STRATEGY
                    value: "single"
                  - name: NVIDIA_VISIBLE_DEVICES
                    value: "all"
                  - name: NVIDIA_DRIVER_CAPABILITIES
                    value: "all"
                volumeMounts:
                  - name: device-plugin
                    mountPath: /var/lib/kubelet/device-plugins
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
            volumes:
              - name: device-plugin
                hostPath:
                  path: /var/lib/kubelet/device-plugins

  # ===========================================================================
  # Bootstrap Script
  # ===========================================================================

  - path: /root/bootstrap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail

      # === Configuration ===
      CLUSTER_NAME="${cluster_name}"
      K8S_ENDPOINT="https://${k8s_api_endpoint}:6443"
      TALOS_VERSION="${talos_version}"

      CONTROL_PLANE_IPS="${control_plane_ips}"
      GPU_WORKER_IPS="${gpu_worker_ips}"
      CPU_WORKER_IPS="${cpu_worker_ips}"

      MIG_ENABLED="${mig_enabled}"
      MIG_PROFILE_CONFIG="${mig_profile_config}"
      MIG_INSTANCE_COUNT="${mig_instance_count}"

      TOTAL_NODES=${total_nodes}

      CONFIG_DIR="/root/talos-config"
      TALOSCONFIG="$CONFIG_DIR/talosconfig"
      LOG="/var/log/talos-bootstrap.log"

      # === Functions ===
      log() { echo "[$(date +'%H:%M:%S')] $*" | tee -a "$LOG"; }
      ok() { log "[OK] $*"; }
      warn() { log "[WARN] $*"; }

      wait_for_network() {
        log "Waiting for network..."
        until ping -c 1 8.8.8.8 >/dev/null 2>&1; do sleep 2; done
        ok "Network ready"
      }

      install_tools() {
        log "Installing tools..."
        command -v talosctl >/dev/null 2>&1 || curl -sL https://talos.dev/install | sh
        if ! command -v kubectl >/dev/null 2>&1; then
          curl -LO "https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl && mv kubectl /usr/local/bin/
        fi
        command -v helm >/dev/null 2>&1 || curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        ok "Tools installed"
      }

      wait_for_nodes() {
        local target=$1
        local timeout=600
        local elapsed=0
        log "Waiting for $target nodes..."
        while [ $elapsed -lt $timeout ]; do
          local ready=$(kubectl get nodes --no-headers 2>/dev/null | grep -c " Ready " || echo "0")
          log "  Ready: $ready/$target"
          [ "$ready" -ge "$target" ] && return 0
          sleep 15
          elapsed=$((elapsed + 15))
        done
        return 1
      }

      # === Main ===
      wait_for_network
      install_tools

      log "=== Talos Bootstrap: $CLUSTER_NAME ==="

      # Wait for all nodes
      ALL_NODES="$CONTROL_PLANE_IPS $GPU_WORKER_IPS $CPU_WORKER_IPS"
      for ip in $ALL_NODES; do
        [ -z "$ip" ] && continue
        log "Waiting for $ip..."
        until nc -z -w5 "$ip" 50000 2>/dev/null; do sleep 5; done
        ok "$ip reachable"
      done

      # Generate Talos config
      mkdir -p "$CONFIG_DIR"
      FIRST_CP=$(echo "$CONTROL_PLANE_IPS" | awk '{print $1}')

      talosctl gen config "$CLUSTER_NAME" "$K8S_ENDPOINT" \
        --output-dir "$CONFIG_DIR" --with-docs=false --with-examples=false

      talosctl --talosconfig "$TALOSCONFIG" config endpoint $FIRST_CP
      talosctl --talosconfig "$TALOSCONFIG" config node $FIRST_CP

      # Patch configs
      talosctl machineconfig patch "$CONFIG_DIR/controlplane.yaml" \
        --patch @/root/patches/cilium.yaml -o "$CONFIG_DIR/controlplane-patched.yaml"

      talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
        --patch @/root/patches/cilium.yaml -o "$CONFIG_DIR/worker-cpu.yaml"

      if [ -n "$GPU_WORKER_IPS" ]; then
        talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
          --patch @/root/patches/cilium.yaml \
          --patch @/root/patches/gpu-worker.yaml \
          -o "$CONFIG_DIR/worker-gpu.yaml"
      fi

      # Apply configs
      log "Applying configurations..."
      for ip in $CONTROL_PLANE_IPS; do
        log "Applying to control plane: $ip"
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/controlplane-patched.yaml"
      done

      for ip in $GPU_WORKER_IPS; do
        log "Applying to GPU worker: $ip"
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-gpu.yaml"
      done

      for ip in $CPU_WORKER_IPS; do
        log "Applying to CPU worker: $ip"
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-cpu.yaml"
      done

      log "Waiting 120s for nodes to configure..."
      sleep 120

      # Bootstrap etcd
      log "Bootstrapping etcd on $FIRST_CP..."
      talosctl --talosconfig "$TALOSCONFIG" bootstrap --nodes "$FIRST_CP"
      sleep 90

      # Get kubeconfig
      talosctl --talosconfig "$TALOSCONFIG" kubeconfig "$CONFIG_DIR/kubeconfig" --nodes "$FIRST_CP" --force
      export KUBECONFIG="$CONFIG_DIR/kubeconfig"

      # Install Cilium
      log "Installing Cilium..."
      helm repo add cilium https://helm.cilium.io/ 2>/dev/null || true
      helm repo update
      helm install cilium cilium/cilium \
        --namespace kube-system \
        --set ipam.mode=kubernetes \
        --set kubeProxyReplacement=true \
        --set k8sServiceHost=${k8s_api_endpoint} \
        --set k8sServicePort=6443 \
        --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
        --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
        --set cgroup.autoMount.enabled=true \
        --set cgroup.hostRoot=/sys/fs/cgroup \
        --timeout 5m

      wait_for_nodes $TOTAL_NODES || warn "Not all nodes ready"

      # === GPU Configuration ===
      if [ -n "$GPU_WORKER_IPS" ]; then
        log "=== GPU Configuration ==="
        
        # Step 1: Deploy base GPU stack (WITHOUT device plugin)
        kubectl apply -f /root/manifests/gpu-stack-base.yaml
        ok "GPU stack base deployed (without device plugin)"

        # Wait for NFD
        log "Waiting for NFD (60s)..."
        sleep 60

        # Label GPU nodes
        for ip in $GPU_WORKER_IPS; do
          NODE=$(kubectl get nodes -o wide --no-headers | grep "$ip" | awk '{print $1}')
          if [ -n "$NODE" ]; then
            kubectl label node "$NODE" nvidia.com/gpu.present=true --overwrite
            ok "Labeled $NODE"
          fi
        done

        # === MIG Configuration (BEFORE device plugin) ===
        if [ "$MIG_ENABLED" = "true" ]; then
          log "=== MIG Configuration ==="
          
          for gpu_ip in $GPU_WORKER_IPS; do
            GPU_NODE=$(kubectl get nodes -o wide --no-headers | grep "$gpu_ip" | awk '{print $1}')
            [ -z "$GPU_NODE" ] && continue

            # Step 2: Check MIG status
            log "Checking MIG status on $GPU_NODE..."
            cat <<MIGCHECK | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: mig-check
  namespace: nvidia-gpu-stack
spec:
  restartPolicy: Never
  runtimeClassName: nvidia
  nodeSelector:
    kubernetes.io/hostname: $GPU_NODE
  tolerations:
    - operator: Exists
  containers:
    - name: check
      image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
      command: ["bash", "-c"]
      args:
        - |
          MIG_MODE=\$(nvidia-smi -i 0 --query-gpu=mig.mode.current --format=csv,noheader 2>/dev/null | tr -d ' ')
          INSTANCES=\$(nvidia-smi mig -lgi 2>/dev/null | grep -c "1g.10gb" || echo "0")
          echo "MIG_STATUS:\$MIG_MODE:\$INSTANCES"
      securityContext:
        privileged: true
MIGCHECK

            sleep 20
            MIG_OUTPUT=$(kubectl -n nvidia-gpu-stack logs mig-check 2>/dev/null | grep "^MIG_STATUS:" | tail -1 || echo "MIG_STATUS:Unknown:0")
            kubectl -n nvidia-gpu-stack delete pod mig-check --ignore-not-found >/dev/null 2>&1
            
            MIG_MODE=$(echo "$MIG_OUTPUT" | cut -d: -f2)
            MIG_INSTANCES=$(echo "$MIG_OUTPUT" | cut -d: -f3)
            log "MIG Mode: $MIG_MODE, Instances: $MIG_INSTANCES"

            # Enable MIG if needed
            if [ "$MIG_MODE" != "Enabled" ]; then
              log "Enabling MIG on $GPU_NODE..."
              cat <<MIGENABLE | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: mig-enable
  namespace: nvidia-gpu-stack
spec:
  restartPolicy: Never
  runtimeClassName: nvidia
  nodeSelector:
    kubernetes.io/hostname: $GPU_NODE
  tolerations:
    - operator: Exists
  containers:
    - name: enable
      image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
      command: ["bash", "-c", "nvidia-smi -pm 1 && nvidia-smi -mig 1 && echo MIG_ENABLED"]
      securityContext:
        privileged: true
MIGENABLE

              sleep 20
              kubectl -n nvidia-gpu-stack logs mig-enable 2>/dev/null || true
              kubectl -n nvidia-gpu-stack delete pod mig-enable --ignore-not-found >/dev/null 2>&1
              
              log "Rebooting $GPU_NODE for MIG activation..."
              talosctl --talosconfig "$TALOSCONFIG" reboot --nodes "$gpu_ip" &
              
              log "Waiting for GPU node to reboot (180s)..."
              sleep 180
              
              wait_for_nodes $TOTAL_NODES || warn "Timeout waiting for nodes"
              kubectl label node "$GPU_NODE" nvidia.com/gpu.present=true --overwrite 2>/dev/null || true
              sleep 30
              MIG_INSTANCES="0"
            fi

            # Create MIG instances if needed
            if [ "$MIG_INSTANCES" -lt "$MIG_INSTANCE_COUNT" ] 2>/dev/null || [ "$MIG_INSTANCES" = "0" ]; then
              log "Creating MIG instances on $GPU_NODE..."
              cat <<MIGCREATE | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: mig-create
  namespace: nvidia-gpu-stack
spec:
  restartPolicy: Never
  runtimeClassName: nvidia
  nodeSelector:
    kubernetes.io/hostname: $GPU_NODE
  tolerations:
    - operator: Exists
  containers:
    - name: create
      image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
      command: ["bash", "-c"]
      args:
        - |
          echo "Creating MIG instances..."
          nvidia-smi mig -cgi $MIG_PROFILE_CONFIG -C 2>&1 || echo "Some instances may already exist"
          echo "=== MIG Instances ==="
          nvidia-smi mig -lgi
          echo "MIG_CREATED"
      securityContext:
        privileged: true
MIGCREATE

              sleep 30
              kubectl -n nvidia-gpu-stack logs mig-create 2>/dev/null || true
              kubectl -n nvidia-gpu-stack delete pod mig-create --ignore-not-found >/dev/null 2>&1
            fi
            
            ok "MIG configured on $GPU_NODE"
          done
        fi

        # Step 4: NOW deploy device plugin (after MIG is fully configured)
        log "Deploying device plugin..."
        kubectl apply -f /root/manifests/device-plugin.yaml
        ok "Device plugin deployed"

        # Wait for device plugin to be ready
        sleep 30
        
        # Final status
        GPU_COUNT=$(kubectl get nodes -o json 2>/dev/null | jq '[.items[].status.allocatable["nvidia.com/gpu"] // "0" | tonumber] | add' || echo "0")
        log "GPU resources available: $GPU_COUNT"
      fi

      log "=== Bootstrap Complete ==="
      ok "Cluster $CLUSTER_NAME is ready!"
      echo ""
      echo "Access: export KUBECONFIG=$CONFIG_DIR/kubeconfig"

  # ===========================================================================
  # Systemd Service
  # ===========================================================================

  - path: /etc/systemd/system/talos-bootstrap.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Talos Cluster Bootstrap
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      User=root
      ExecStart=/root/bootstrap.sh
      StandardOutput=journal+console
      StandardError=journal+console
      RemainAfterExit=yes
      TimeoutStartSec=3600

      [Install]
      WantedBy=multi-user.target

runcmd:
  - mkdir -p /root/patches /root/manifests
  - systemctl daemon-reload
  - systemctl enable --now talos-bootstrap.service
