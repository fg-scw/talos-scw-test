#cloud-config
# =============================================================================
# Talos Cluster Bootstrap - Automatic MIG Configuration
# =============================================================================

package_update: true
package_upgrade: false

packages:
  - curl
  - wget
  - netcat-openbsd
  - jq

write_files:
  # ===========================================================================
  # Talos Configuration Patches
  # ===========================================================================

  - path: /root/patches/cilium.yaml
    permissions: '0644'
    content: |
      cluster:
        proxy:
          disabled: true
        network:
          cni:
            name: none

  - path: /root/patches/gpu-worker.yaml
    permissions: '0644'
    content: |
      machine:
        kernel:
          modules:
            - name: nvidia
            - name: nvidia_uvm
            - name: nvidia_drm
            - name: nvidia_modeset
        sysctls:
          net.core.bpf_jit_harden: "1"

  # ===========================================================================
  # GPU Stack Manifest (CDI Mode for Talos)
  # ===========================================================================

  - path: /root/manifests/gpu-stack.yaml
    permissions: '0644'
    content: |
      ---
      apiVersion: v1
      kind: Namespace
      metadata:
        name: nvidia-gpu-stack
        labels:
          pod-security.kubernetes.io/enforce: privileged
          pod-security.kubernetes.io/audit: privileged
          pod-security.kubernetes.io/warn: privileged
      ---
      apiVersion: node.k8s.io/v1
      kind: RuntimeClass
      metadata:
        name: nvidia
      handler: nvidia
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mig-config
        namespace: nvidia-gpu-stack
      data:
        MIG_ENABLED: "${mig_enabled}"
        MIG_PROFILE: "${mig_profile}"
        MIG_PROFILE_CONFIG: "${mig_profile_config}"
        MIG_INSTANCE_COUNT: "${mig_instance_count}"
      ---
      apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      metadata:
        name: nodefeatures.nfd.k8s-sigs.io
      spec:
        group: nfd.k8s-sigs.io
        names:
          kind: NodeFeature
          listKind: NodeFeatureList
          plural: nodefeatures
          singular: nodefeature
        scope: Namespaced
        versions:
          - name: v1alpha1
            served: true
            storage: true
            schema:
              openAPIV3Schema:
                type: object
                properties:
                  spec:
                    type: object
                    x-kubernetes-preserve-unknown-fields: true
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: nfd-master
        namespace: nvidia-gpu-stack
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: nfd-worker
        namespace: nvidia-gpu-stack
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: gpu-operator
        namespace: nvidia-gpu-stack
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: nfd-master
      rules:
        - apiGroups: [""]
          resources: ["nodes", "nodes/status"]
          verbs: ["get", "list", "watch", "patch", "update"]
        - apiGroups: ["nfd.k8s-sigs.io"]
          resources: ["nodefeatures"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["coordination.k8s.io"]
          resources: ["leases"]
          verbs: ["create", "get", "update"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: nfd-master
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: nfd-master
      subjects:
        - kind: ServiceAccount
          name: nfd-master
          namespace: nvidia-gpu-stack
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: nfd-worker
      rules:
        - apiGroups: ["nfd.k8s-sigs.io"]
          resources: ["nodefeatures"]
          verbs: ["create", "get", "update"]
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "watch"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: nfd-worker
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: nfd-worker
      subjects:
        - kind: ServiceAccount
          name: nfd-worker
          namespace: nvidia-gpu-stack
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: gpu-operator
      rules:
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "watch", "patch", "update"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: gpu-operator
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: gpu-operator
      subjects:
        - kind: ServiceAccount
          name: gpu-operator
          namespace: nvidia-gpu-stack
      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: nfd-master
        namespace: nvidia-gpu-stack
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: nfd-master
        template:
          metadata:
            labels:
              app: nfd-master
          spec:
            serviceAccountName: nfd-master
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
            containers:
              - name: nfd-master
                image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
                command: ["nfd-master"]
                args: ["-port=8080"]
                env:
                  - name: NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                ports:
                  - containerPort: 8080
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: nfd-master
        namespace: nvidia-gpu-stack
      spec:
        selector:
          app: nfd-master
        ports:
          - port: 8080
            name: grpc
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nfd-worker
        namespace: nvidia-gpu-stack
      spec:
        selector:
          matchLabels:
            app: nfd-worker
        template:
          metadata:
            labels:
              app: nfd-worker
          spec:
            serviceAccountName: nfd-worker
            dnsPolicy: ClusterFirstWithHostNet
            tolerations:
              - operator: Exists
            containers:
              - name: nfd-worker
                image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
                command: ["nfd-worker"]
                args: ["-server=nfd-master.nvidia-gpu-stack.svc.cluster.local:8080"]
                env:
                  - name: NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                volumeMounts:
                  - name: host-boot
                    mountPath: /host-boot
                    readOnly: true
                  - name: host-sys
                    mountPath: /host-sys
                    readOnly: true
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
            volumes:
              - name: host-boot
                hostPath:
                  path: /boot
              - name: host-sys
                hostPath:
                  path: /sys
      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: gpu-labeler
        namespace: nvidia-gpu-stack
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: gpu-labeler
        template:
          metadata:
            labels:
              app: gpu-labeler
          spec:
            serviceAccountName: gpu-operator
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
            containers:
              - name: labeler
                image: bitnami/kubectl:latest
                command: ["/bin/bash", "-c"]
                args:
                  - |
                    echo "GPU Labeler started"
                    while true; do
                      for node in $(kubectl get nodes -l feature.node.kubernetes.io/pci-10de.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
                        CURRENT=$(kubectl get node $node -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' 2>/dev/null)
                        if [ "$CURRENT" != "true" ]; then
                          echo "Labeling $node"
                          kubectl label node $node nvidia.com/gpu.present=true --overwrite
                        fi
                      done
                      sleep 30
                    done
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 50m
                    memory: 64Mi
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nvidia-device-plugin
        namespace: nvidia-gpu-stack
      spec:
        selector:
          matchLabels:
            app: nvidia-device-plugin
        template:
          metadata:
            labels:
              app: nvidia-device-plugin
          spec:
            runtimeClassName: nvidia
            nodeSelector:
              nvidia.com/gpu.present: "true"
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            priorityClassName: system-node-critical
            containers:
              - name: nvidia-device-plugin
                image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
                securityContext:
                  privileged: true
                env:
                  - name: FAIL_ON_INIT_ERROR
                    value: "false"
                  - name: MIG_STRATEGY
                    value: "single"
                  - name: DEVICE_LIST_STRATEGY
                    value: "cdi-annotations"
                  - name: CDI_ANNOTATION_PREFIX
                    value: "cdi.k8s.io/"
                  - name: NVIDIA_CTK_PATH
                    value: "/usr/local/bin/nvidia-ctk"
                  - name: PASS_DEVICE_SPECS
                    value: "true"
                  - name: NVIDIA_VISIBLE_DEVICES
                    value: "all"
                  - name: NVIDIA_DRIVER_CAPABILITIES
                    value: "all"
                volumeMounts:
                  - name: device-plugin
                    mountPath: /var/lib/kubelet/device-plugins
                  - name: cdi
                    mountPath: /var/run/cdi
                resources:
                  requests:
                    cpu: 10m
                    memory: 32Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
            volumes:
              - name: device-plugin
                hostPath:
                  path: /var/lib/kubelet/device-plugins
              - name: cdi
                hostPath:
                  path: /var/run/cdi
                  type: DirectoryOrCreate

  # ===========================================================================
  # Bootstrap Script
  # ===========================================================================

  - path: /root/bootstrap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail

      # === Configuration (injected by Terraform) ===
      CLUSTER_NAME="${cluster_name}"
      K8S_ENDPOINT="https://${k8s_api_endpoint}:6443"
      TALOS_VERSION="${talos_version}"

      CONTROL_PLANE_IPS="${control_plane_ips}"
      GPU_WORKER_IPS="${gpu_worker_ips}"
      CPU_WORKER_IPS="${cpu_worker_ips}"

      MIG_ENABLED="${mig_enabled}"
      MIG_PROFILE="${mig_profile}"
      MIG_PROFILE_CONFIG="${mig_profile_config}"
      MIG_INSTANCE_COUNT="${mig_instance_count}"

      # Pre-calculated by Terraform
      TOTAL_NODES=${total_nodes}

      CONFIG_DIR="/root/talos-config"
      TALOSCONFIG="$CONFIG_DIR/talosconfig"
      LOG="/var/log/talos-bootstrap.log"

      # === Functions ===
      log() { echo "[`date +'%H:%M:%S'`] $*" | tee -a "$LOG"; }
      ok() { log "[OK] $*"; }
      warn() { log "[WARN] $*"; }
      fail() { log "[ERROR] $*"; }

      wait_for_network() {
        log "Waiting for network..."
        until ping -c 1 8.8.8.8 >/dev/null 2>&1; do sleep 2; done
        ok "Network ready"
      }

      install_tools() {
        log "Installing tools..."
        if ! command -v talosctl >/dev/null 2>&1; then
          curl -sL https://talos.dev/install | sh
        fi
        if ! command -v kubectl >/dev/null 2>&1; then
          KUBECTL_VERSION=`curl -sL https://dl.k8s.io/release/stable.txt`
          curl -LO "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
          chmod +x kubectl && mv kubectl /usr/local/bin/
        fi
        if ! command -v helm >/dev/null 2>&1; then
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        fi
        ok "Tools installed"
      }

      wait_for_nodes() {
        local target=$1
        local timeout=600
        local elapsed=0
        log "Waiting for $target nodes..."
        while [ $elapsed -lt $timeout ]; do
          local ready=`kubectl get nodes --no-headers 2>/dev/null | grep -c " Ready " || echo 0`
          log "  Ready: $ready/$target"
          if [ "$ready" -ge "$target" ]; then
            return 0
          fi
          sleep 15
          elapsed=$((elapsed + 15))
        done
        return 1
      }

      # === Main ===
      wait_for_network
      install_tools

      log "=== Talos Bootstrap: $CLUSTER_NAME ==="

      # Wait for all nodes to be reachable
      ALL_NODES="$CONTROL_PLANE_IPS $GPU_WORKER_IPS $CPU_WORKER_IPS"
      for ip in $ALL_NODES; do
        if [ -z "$ip" ]; then continue; fi
        log "Waiting for $ip..."
        until nc -z -w5 "$ip" 50000 2>/dev/null; do sleep 5; done
        ok "$ip reachable"
      done

      # Generate Talos config
      mkdir -p "$CONFIG_DIR"
      FIRST_CP=`echo "$CONTROL_PLANE_IPS" | awk '{print $1}'`

      talosctl gen config "$CLUSTER_NAME" "$K8S_ENDPOINT" \
        --output-dir "$CONFIG_DIR" --with-docs=false --with-examples=false

      talosctl --talosconfig "$TALOSCONFIG" config endpoint $FIRST_CP
      talosctl --talosconfig "$TALOSCONFIG" config node $FIRST_CP

      # Patch configs
      talosctl machineconfig patch "$CONFIG_DIR/controlplane.yaml" \
        --patch @/root/patches/cilium.yaml -o "$CONFIG_DIR/controlplane-patched.yaml"

      talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
        --patch @/root/patches/cilium.yaml -o "$CONFIG_DIR/worker-cpu.yaml"

      if [ -n "$GPU_WORKER_IPS" ]; then
        talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
          --patch @/root/patches/cilium.yaml \
          --patch @/root/patches/gpu-worker.yaml \
          -o "$CONFIG_DIR/worker-gpu.yaml"
      fi

      # Apply configs
      log "Applying configurations..."

      for ip in $CONTROL_PLANE_IPS; do
        log "Applying config to control plane: $ip"
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/controlplane-patched.yaml"
      done

      for ip in $GPU_WORKER_IPS; do
        log "Applying config to GPU worker: $ip"
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-gpu.yaml"
      done

      for ip in $CPU_WORKER_IPS; do
        log "Applying config to CPU worker: $ip"
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-cpu.yaml"
      done

      log "Waiting 120s for nodes to configure..."
      sleep 120

      # Bootstrap etcd
      log "Bootstrapping etcd on $FIRST_CP..."
      talosctl --talosconfig "$TALOSCONFIG" bootstrap --nodes "$FIRST_CP"

      log "Waiting 90s for cluster..."
      sleep 90

      # Get kubeconfig
      talosctl --talosconfig "$TALOSCONFIG" kubeconfig "$CONFIG_DIR/kubeconfig" --nodes "$FIRST_CP" --force
      export KUBECONFIG="$CONFIG_DIR/kubeconfig"

      # Install Cilium
      log "Installing Cilium..."
      helm repo add cilium https://helm.cilium.io/ 2>/dev/null || true
      helm repo update
      helm install cilium cilium/cilium \
        --namespace kube-system \
        --set ipam.mode=kubernetes \
        --set kubeProxyReplacement=true \
        --set k8sServiceHost=${k8s_api_endpoint} \
        --set k8sServicePort=6443 \
        --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
        --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
        --set cgroup.autoMount.enabled=true \
        --set cgroup.hostRoot=/sys/fs/cgroup \
        --timeout 5m

      wait_for_nodes $TOTAL_NODES 600 || warn "Not all nodes ready"

      # === GPU Configuration ===
      if [ -n "$GPU_WORKER_IPS" ]; then
        log "=== GPU Configuration ==="
        kubectl label namespace kube-system pod-security.kubernetes.io/enforce=privileged --overwrite

        # Apply GPU stack
        kubectl apply -f /root/manifests/gpu-stack.yaml
        ok "GPU stack deployed"

        # Wait for NFD to label nodes
        log "Waiting for NFD to detect GPUs (60s)..."
        sleep 60

        # Pre-label GPU nodes
        for ip in $GPU_WORKER_IPS; do
          NODE=`kubectl get nodes -o wide --no-headers | grep "$ip" | awk '{print $1}'`
          if [ -n "$NODE" ]; then
            kubectl label node "$NODE" nvidia.com/gpu.present=true --overwrite
            ok "Labeled $NODE"
          fi
        done

        # === MIG Activation ===
        if [ "$MIG_ENABLED" = "true" ]; then
          log "=== MIG Activation ==="
          
          MIG_NEEDS_REBOOT="false"
          
          for gpu_ip in $GPU_WORKER_IPS; do
            GPU_NODE=`kubectl get nodes -o wide --no-headers | grep "$gpu_ip" | awk '{print $1}'`
            if [ -z "$GPU_NODE" ]; then continue; fi

            log "Checking MIG status on $GPU_NODE ($gpu_ip)..."
            
            POD_SUFFIX=`echo $gpu_ip | tr '.' '-'`

            # Create privileged pod to check/enable MIG
            kubectl apply -f - <<MIGPOD
      apiVersion: v1
      kind: Pod
      metadata:
        name: mig-enabler-$POD_SUFFIX
        namespace: nvidia-gpu-stack
      spec:
        restartPolicy: Never
        runtimeClassName: nvidia
        nodeSelector:
          kubernetes.io/hostname: $GPU_NODE
        tolerations:
          - operator: Exists
        containers:
          - name: enabler
            image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
            command: ["/bin/bash", "-c"]
            args:
              - |
                echo "=== MIG Enabler ==="
                if ! nvidia-smi 2>/dev/null; then
                  echo "NVIDIA drivers not ready"
                  exit 1
                fi
                MIG_MODE=\$(nvidia-smi -i 0 --query-gpu=mig.mode.current --format=csv,noheader 2>/dev/null || echo "N/A")
                echo "Current MIG mode: \$MIG_MODE"
                if [ "\$MIG_MODE" != "Enabled" ]; then
                  echo "Enabling MIG mode..."
                  nvidia-smi -mig 1 || true
                  echo "MIG_PENDING"
                else
                  echo "MIG_ALREADY_ENABLED"
                fi
            securityContext:
              privileged: true
            resources:
              limits:
                nvidia.com/gpu: 1
      MIGPOD

            log "Waiting for mig-enabler pod..."
            sleep 45

            # Check result
            MIG_RESULT=`kubectl -n nvidia-gpu-stack logs mig-enabler-$POD_SUFFIX 2>/dev/null | tail -1 || echo "UNKNOWN"`
            log "MIG result: $MIG_RESULT"

            kubectl -n nvidia-gpu-stack delete pod mig-enabler-$POD_SUFFIX --ignore-not-found >/dev/null 2>&1

            if [ "$MIG_RESULT" = "MIG_PENDING" ]; then
              log "MIG enabled, will reboot $GPU_NODE..."
              MIG_NEEDS_REBOOT="true"
              talosctl --talosconfig "$TALOSCONFIG" reboot --nodes "$gpu_ip" &
              sleep 5
            elif [ "$MIG_RESULT" = "MIG_ALREADY_ENABLED" ]; then
              ok "MIG already enabled on $GPU_NODE"
            fi
          done

          # Wait for GPU nodes to come back after reboot
          if [ "$MIG_NEEDS_REBOOT" = "true" ]; then
            log "Waiting for GPU nodes to reboot (180s)..."
            sleep 180

            # Wait for nodes to be ready again
            wait_for_nodes $TOTAL_NODES 300 || warn "Timeout waiting for nodes"
          fi

          # Configure MIG instances
          for gpu_ip in $GPU_WORKER_IPS; do
            GPU_NODE=`kubectl get nodes -o wide --no-headers 2>/dev/null | grep "$gpu_ip" | awk '{print $1}'`
            if [ -z "$GPU_NODE" ]; then continue; fi

            log "Configuring MIG instances on $GPU_NODE..."
            
            POD_SUFFIX=`echo $gpu_ip | tr '.' '-'`

            kubectl apply -f - <<MIGCFG
      apiVersion: v1
      kind: Pod
      metadata:
        name: mig-config-$POD_SUFFIX
        namespace: nvidia-gpu-stack
      spec:
        restartPolicy: Never
        runtimeClassName: nvidia
        nodeSelector:
          kubernetes.io/hostname: $GPU_NODE
        tolerations:
          - operator: Exists
        containers:
          - name: config
            image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
            command: ["/bin/bash", "-c"]
            args:
              - |
                echo "=== MIG Configuration ==="
                nvidia-smi -L
                MIG_MODE=\$(nvidia-smi -i 0 --query-gpu=mig.mode.current --format=csv,noheader)
                echo "MIG Mode: \$MIG_MODE"
                if [ "\$MIG_MODE" = "Enabled" ]; then
                  EXISTING=\$(nvidia-smi mig -lgi 2>/dev/null | grep -c "MIG" || echo 0)
                  if [ "\$EXISTING" -eq 0 ]; then
                    echo "Creating MIG instances: $MIG_PROFILE_CONFIG"
                    nvidia-smi mig -cgi $MIG_PROFILE_CONFIG -C
                    nvidia-smi mig -lgi
                  else
                    echo "MIG instances already exist: \$EXISTING"
                    nvidia-smi mig -lgi
                  fi
                fi
            securityContext:
              privileged: true
            resources:
              limits:
                nvidia.com/gpu: 1
      MIGCFG

            log "Waiting for mig-config pod..."
            sleep 45
            kubectl -n nvidia-gpu-stack logs mig-config-$POD_SUFFIX 2>/dev/null || true
            kubectl -n nvidia-gpu-stack delete pod mig-config-$POD_SUFFIX --ignore-not-found >/dev/null 2>&1
          done

          # Restart device plugin to pick up MIG devices
          log "Restarting device plugin..."
          kubectl -n nvidia-gpu-stack delete pod -l app=nvidia-device-plugin --ignore-not-found
          sleep 30
        fi

        # Final GPU status
        sleep 30
        GPU_COUNT=`kubectl get nodes -o json 2>/dev/null | jq '[.items[].status.allocatable["nvidia.com/gpu"] // "0" | tonumber] | add' || echo 0`
        log "GPU resources available: $GPU_COUNT"
      fi

      log "=== Bootstrap Complete ==="
      ok "Cluster $CLUSTER_NAME is ready!"
      echo ""
      echo "To access the cluster:"
      echo "  export KUBECONFIG=$CONFIG_DIR/kubeconfig"
      echo "  kubectl get nodes"

  # ===========================================================================
  # Systemd Service
  # ===========================================================================

  - path: /etc/systemd/system/talos-bootstrap.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Talos Cluster Bootstrap
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      User=root
      ExecStart=/root/bootstrap.sh
      StandardOutput=journal+console
      StandardError=journal+console
      RemainAfterExit=yes
      TimeoutStartSec=3600

      [Install]
      WantedBy=multi-user.target

runcmd:
  - mkdir -p /root/patches /root/manifests
  - systemctl daemon-reload
  - systemctl enable --now talos-bootstrap.service
