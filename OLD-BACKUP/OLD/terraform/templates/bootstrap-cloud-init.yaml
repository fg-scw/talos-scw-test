#cloud-config

package_update: true
package_upgrade: false

packages:
  - curl
  - wget
  - jq
  - netcat-openbsd

write_files:
  # Cilium patch (disable kube-proxy)
  - path: /root/cilium-patch.yaml
    permissions: '0644'
    content: |
      cluster:
        proxy:
          disabled: true
        network:
          cni:
            name: none

  # GPU worker patch - CRITICAL: includes both kernel modules AND containerd runtime config
  # Reference: https://www.talos.dev/v1.11/talos-guides/configuration/nvidia-gpu/
  - path: /root/gpu-worker-patch.yaml
    permissions: '0644'
    content: |
      machine:
        kernel:
          modules:
            - name: nvidia
            - name: nvidia_uvm
            - name: nvidia_drm
            - name: nvidia_modeset
        sysctls:
          net.core.bpf_jit_harden: "1"
        files:
          - content: |
              [plugins]
              [plugins."io.containerd.cri.v1.runtime"]
              [plugins."io.containerd.cri.v1.runtime".containerd]
              default_runtime_name = "nvidia"
            path: /etc/cri/conf.d/20-customization.part
            op: create

  # NVIDIA RuntimeClass
  - path: /root/nvidia-runtime-class.yaml
    permissions: '0644'
    content: |
      apiVersion: node.k8s.io/v1
      kind: RuntimeClass
      metadata:
        name: nvidia
      handler: nvidia

  # NVIDIA Device Plugin - Static manifest v0.14.5 (compatible with Talos)
  # Note: Helm chart v0.15.0+ has scheduling issues on Talos (see issue #12103)
  # MIG_STRATEGY=single exposes each MIG instance as a separate GPU resource
  - path: /root/nvidia-device-plugin.yaml
    permissions: '0644'
    content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: nvidia-device-plugin
        labels:
          pod-security.kubernetes.io/enforce: privileged
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nvidia-device-plugin-daemonset
        namespace: nvidia-device-plugin
      spec:
        selector:
          matchLabels:
            name: nvidia-device-plugin-ds
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              name: nvidia-device-plugin-ds
          spec:
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
            priorityClassName: system-node-critical
            runtimeClassName: nvidia
            containers:
              - image: nvcr.io/nvidia/k8s-device-plugin:v0.14.5
                name: nvidia-device-plugin-ctr
                env:
                  - name: FAIL_ON_INIT_ERROR
                    value: "false"
                  # MIG Strategy Configuration:
                  # - "none": Ignores MIG, exposes whole GPU (default)
                  # - "single": Exposes each MIG instance as separate GPU
                  # - "mixed": Supports both whole GPU and MIG instances
                  - name: MIG_STRATEGY
                    value: "single"
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop: ["ALL"]
                volumeMounts:
                  - name: device-plugin
                    mountPath: /var/lib/kubelet/device-plugins
            volumes:
              - name: device-plugin
                hostPath:
                  path: /var/lib/kubelet/device-plugins
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: nvidia.com/gpu.present
                          operator: In
                          values:
                            - "true"
                    - matchExpressions:
                        - key: feature.node.kubernetes.io/pci-10de.present
                          operator: In
                          values:
                            - "true"

  # GPU Feature Discovery for node labeling
  - path: /root/gpu-feature-discovery.yaml
    permissions: '0644'
    content: |
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: gpu-feature-discovery
        namespace: nvidia-device-plugin
        labels:
          app.kubernetes.io/name: gpu-feature-discovery
      spec:
        selector:
          matchLabels:
            app.kubernetes.io/name: gpu-feature-discovery
        template:
          metadata:
            labels:
              app.kubernetes.io/name: gpu-feature-discovery
          spec:
            serviceAccountName: gpu-feature-discovery
            runtimeClassName: nvidia
            containers:
              - image: nvcr.io/nvidia/gpu-feature-discovery:v0.8.2
                name: gpu-feature-discovery
                volumeMounts:
                  - name: output-dir
                    mountPath: /etc/kubernetes/node-feature-discovery/features.d
                  - name: host-sys
                    mountPath: /sys
                env:
                  - name: GFD_SLEEP_INTERVAL
                    value: "60s"
                  - name: GFD_FAIL_ON_INIT_ERROR
                    value: "false"
                  # Enable MIG mode detection for proper labeling
                  - name: GFD_MIG_STRATEGY
                    value: "single"
                securityContext:
                  privileged: true
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            volumes:
              - name: output-dir
                hostPath:
                  path: /etc/kubernetes/node-feature-discovery/features.d
              - name: host-sys
                hostPath:
                  path: /sys
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: nvidia.com/gpu.present
                          operator: In
                          values:
                            - "true"
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: gpu-feature-discovery
        namespace: nvidia-device-plugin
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: gpu-feature-discovery
      rules:
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "patch", "update"]
        - apiGroups: ["nfd.k8s-sigs.io"]
          resources: ["nodefeatures"]
          verbs: ["create", "get", "update"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: gpu-feature-discovery
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: gpu-feature-discovery
      subjects:
        - kind: ServiceAccount
          name: gpu-feature-discovery
          namespace: nvidia-device-plugin

  # MIG ConfigMap (for optional MIG support)
  - path: /root/mig-config.yaml
    permissions: '0644'
    content: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mig-parted-config
        namespace: nvidia-device-plugin
      data:
        config.yaml: |
          version: v1
          mig-configs:
            all-disabled:
              - devices: all
                mig-enabled: false
            all-1g.10gb:
              - devices: all
                mig-enabled: true
                mig-devices:
                  "1g.10gb": 7
            all-2g.20gb:
              - devices: all
                mig-enabled: true
                mig-devices:
                  "2g.20gb": 3
            all-3g.40gb:
              - devices: all
                mig-enabled: true
                mig-devices:
                  "3g.40gb": 2
            mixed-40-20:
              - devices: all
                mig-enabled: true
                mig-devices:
                  "3g.40gb": 1
                  "2g.20gb": 2

  # Bootstrap script
  - path: /root/bootstrap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail
      
      # =======================================================================
      # Configuration
      # =======================================================================
      CLUSTER_NAME="${cluster_name}"
      K8S_ENDPOINT="https://${k8s_api_endpoint}:6443"
      CONTROL_PLANE_IPS=(${control_plane_ips})
      GPU_WORKER_IPS=(${gpu_worker_ips})
      CPU_WORKER_IPS=(${cpu_worker_ips})
      TOTAL_NODES=$((${control_plane_count} + ${gpu_worker_count} + ${cpu_worker_count}))
      CONFIG_DIR="/root/talos-config"
      TALOSCONFIG="$CONFIG_DIR/talosconfig"
      LOG_FILE="/var/log/talos-bootstrap.log"
      ENABLE_GPU_MIG="${enable_gpu_mig}"
      GPU_MIG_PROFILE="${gpu_mig_profile}"
      
      # Combine worker IPs
      ALL_WORKER_IPS=()
      [[ $${#GPU_WORKER_IPS[@]} -gt 0 ]] && ALL_WORKER_IPS+=("$${GPU_WORKER_IPS[@]}")
      [[ $${#CPU_WORKER_IPS[@]} -gt 0 ]] && ALL_WORKER_IPS+=("$${CPU_WORKER_IPS[@]}")
      
      # =======================================================================
      # Functions
      # =======================================================================
      log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"; }
      log_ok() { log "✓ $*"; }
      log_warn() { log "⚠️ $*"; }
      log_error() { log "✗ $*"; }
      
      wait_for_nodes_ready() {
        local target=$1
        local timeout=$2
        local count=0
        
        log "Waiting for $target nodes to be Ready (timeout: $${timeout}s)..."
        while [ $count -lt $timeout ]; do
          local ready=$(kubectl get nodes --no-headers 2>/dev/null | grep -c " Ready " || echo "0")
          log "  Ready: $ready/$target"
          if [ "$ready" -ge "$target" ]; then
            log_ok "All $target nodes are Ready"
            return 0
          fi
          sleep 10
          count=$((count + 10))
        done
        log_warn "Timeout waiting for nodes"
        return 1
      }
      
      # =======================================================================
      # Main
      # =======================================================================
      log "=========================================="
      log "Talos Cluster Bootstrap - $CLUSTER_NAME"
      log "=========================================="
      log "Endpoint: $K8S_ENDPOINT"
      log "Control Planes: $${CONTROL_PLANE_IPS[*]}"
      log "GPU Workers: $${GPU_WORKER_IPS[*]:-none}"
      log "CPU Workers: $${CPU_WORKER_IPS[*]:-none}"
      log "Total nodes: $TOTAL_NODES"
      log "GPU MIG: $ENABLE_GPU_MIG (profile: $GPU_MIG_PROFILE)"
      
      # -----------------------------------------------------------------------
      # Wait for nodes to be reachable
      # -----------------------------------------------------------------------
      log "Waiting for nodes to be reachable..."
      for ip in "$${CONTROL_PLANE_IPS[@]}" "$${ALL_WORKER_IPS[@]}"; do
        log "  Checking $ip:50000..."
        until nc -z -w5 "$ip" 50000 2>/dev/null; do sleep 5; done
        log_ok "$ip reachable"
      done
      
      # -----------------------------------------------------------------------
      # Generate Talos configs
      # -----------------------------------------------------------------------
      log "Generating Talos configurations..."
      mkdir -p "$CONFIG_DIR"
      talosctl gen config "$CLUSTER_NAME" "$K8S_ENDPOINT" \
        --output-dir "$CONFIG_DIR" \
        --with-docs=false \
        --with-examples=false
      
      talosctl --talosconfig "$TALOSCONFIG" config endpoint $${CONTROL_PLANE_IPS[@]}
      talosctl --talosconfig "$TALOSCONFIG" config node $${CONTROL_PLANE_IPS[0]}
      
      # -----------------------------------------------------------------------
      # Create patched configs
      # -----------------------------------------------------------------------
      log "Creating patched configurations..."
      
      # Control plane: Cilium patch
      talosctl machineconfig patch "$CONFIG_DIR/controlplane.yaml" \
        --patch @/root/cilium-patch.yaml \
        -o "$CONFIG_DIR/controlplane-patched.yaml"
      
      # CPU workers: Cilium patch
      talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
        --patch @/root/cilium-patch.yaml \
        -o "$CONFIG_DIR/worker-cpu-patched.yaml"
      
      # GPU workers: Cilium + GPU full patch (modules + containerd nvidia runtime)
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        log "Creating GPU worker config with NVIDIA runtime..."
        talosctl machineconfig patch "$CONFIG_DIR/worker.yaml" \
          --patch @/root/cilium-patch.yaml \
          --patch @/root/gpu-worker-patch.yaml \
          -o "$CONFIG_DIR/worker-gpu-patched.yaml"
        
        log "GPU worker patch includes:"
        log "  - NVIDIA kernel modules (nvidia, nvidia_uvm, nvidia_drm, nvidia_modeset)"
        log "  - Containerd default_runtime_name = nvidia"
      fi
      
      # -----------------------------------------------------------------------
      # Apply configs to nodes
      # -----------------------------------------------------------------------
      log "Applying configurations to control planes..."
      for ip in "$${CONTROL_PLANE_IPS[@]}"; do
        talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/controlplane-patched.yaml"
        log_ok "Configured $ip (control-plane)"
      done
      
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        log "Applying configurations to GPU workers..."
        for ip in "$${GPU_WORKER_IPS[@]}"; do
          talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-gpu-patched.yaml"
          log_ok "Configured $ip (GPU worker with NVIDIA runtime)"
        done
      fi
      
      if [ $${#CPU_WORKER_IPS[@]} -gt 0 ]; then
        log "Applying configurations to CPU workers..."
        for ip in "$${CPU_WORKER_IPS[@]}"; do
          talosctl apply-config --insecure --nodes "$ip" --file "$CONFIG_DIR/worker-cpu-patched.yaml"
          log_ok "Configured $ip (CPU worker)"
        done
      fi
      
      # -----------------------------------------------------------------------
      # Bootstrap cluster
      # -----------------------------------------------------------------------
      log "Waiting 120s for nodes to apply configuration..."
      sleep 120
      
      log "Bootstrapping etcd on $${CONTROL_PLANE_IPS[0]}..."
      talosctl --talosconfig "$TALOSCONFIG" bootstrap --nodes "$${CONTROL_PLANE_IPS[0]}"
      log_ok "etcd bootstrap initiated"
      
      log "Waiting 90s for cluster initialization..."
      sleep 90
      
      # -----------------------------------------------------------------------
      # Get kubeconfig
      # -----------------------------------------------------------------------
      log "Fetching kubeconfig..."
      talosctl --talosconfig "$TALOSCONFIG" kubeconfig "$CONFIG_DIR/kubeconfig" \
        --nodes "$${CONTROL_PLANE_IPS[0]}" --force
      export KUBECONFIG="$CONFIG_DIR/kubeconfig"
      
      # Wait for API
      log "Waiting for Kubernetes API..."
      for i in {1..60}; do
        if kubectl get nodes &>/dev/null; then
          log_ok "API server ready"
          break
        fi
        sleep 10
      done
      
      kubectl get nodes -o wide || true
      
      # -----------------------------------------------------------------------
      # Install Cilium
      # -----------------------------------------------------------------------
      log "Installing Cilium CNI..."
      helm repo add cilium https://helm.cilium.io/ 2>/dev/null || true
      helm repo update
      
      helm install cilium cilium/cilium \
        --namespace kube-system \
        --set ipam.mode=kubernetes \
        --set kubeProxyReplacement=true \
        --set k8sServiceHost=${k8s_api_endpoint} \
        --set k8sServicePort=6443 \
        --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
        --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
        --set cgroup.autoMount.enabled=true \
        --set cgroup.hostRoot=/sys/fs/cgroup \
        --timeout 5m || log_warn "Cilium install timed out"
      
      log_ok "Cilium installed"
      
      # Wait for nodes
      wait_for_nodes_ready $TOTAL_NODES 600 || true
      
      # -----------------------------------------------------------------------
      # GPU Stack (if GPU workers exist)
      # -----------------------------------------------------------------------
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        log "=========================================="
        log "Installing GPU Stack..."
        log "=========================================="
        
        # Create namespace with privileged PSS
        kubectl create namespace nvidia-device-plugin --dry-run=client -o yaml | kubectl apply -f -
        kubectl label namespace nvidia-device-plugin pod-security.kubernetes.io/enforce=privileged --overwrite
        kubectl label namespace kube-system pod-security.kubernetes.io/enforce=privileged --overwrite
        log_ok "Namespaces labeled as privileged"
        
        # RuntimeClass
        kubectl apply -f /root/nvidia-runtime-class.yaml
        log_ok "RuntimeClass 'nvidia' created"
        
        # Pre-label GPU nodes BEFORE installing NFD/device-plugin
        log "Pre-labeling GPU nodes..."
        sleep 30  # Wait for nodes to be fully ready
        for ip in "$${GPU_WORKER_IPS[@]}"; do
          NODE_NAME=$(kubectl get nodes -o wide --no-headers | grep "$ip" | awk '{print $1}' || true)
          if [ -n "$NODE_NAME" ]; then
            kubectl label node "$NODE_NAME" nvidia.com/gpu.present=true --overwrite || true
            kubectl label node "$NODE_NAME" feature.node.kubernetes.io/pci-10de.present=true --overwrite || true
            log_ok "Labeled $NODE_NAME with nvidia.com/gpu.present=true"
          else
            log_warn "Could not find node for IP $ip"
          fi
        done
        
        # Verify GPU node has NVIDIA extensions and modules
        log "Verifying GPU node configuration..."
        GPU_IP="$${GPU_WORKER_IPS[0]}"
        
        log "Checking NVIDIA extensions on $GPU_IP..."
        if talosctl -n "$GPU_IP" get extensions 2>/dev/null | grep -q "nvidia"; then
          log_ok "NVIDIA extensions found"
          talosctl -n "$GPU_IP" get extensions 2>/dev/null | grep nvidia || true
        else
          log_error "NVIDIA extensions NOT found! Please rebuild GPU image."
        fi
        
        log "Checking NVIDIA kernel modules on $GPU_IP..."
        if talosctl -n "$GPU_IP" read /proc/modules 2>/dev/null | grep -q "^nvidia "; then
          log_ok "NVIDIA kernel modules loaded"
          talosctl -n "$GPU_IP" read /proc/modules 2>/dev/null | grep nvidia || true
        else
          log_warn "NVIDIA kernel modules not loaded yet - may need more time"
        fi
        
        # Check containerd runtime configuration
        log "Checking containerd runtime configuration..."
        if talosctl -n "$GPU_IP" read /etc/cri/conf.d/20-customization.part 2>/dev/null | grep -q "nvidia"; then
          log_ok "Containerd nvidia runtime configured"
        else
          log_error "Containerd nvidia runtime NOT configured! This will cause container failures."
        fi
        
        # Install NFD (Node Feature Discovery) - needed for automatic GPU labeling
        log "Installing Node Feature Discovery..."
        helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts 2>/dev/null || true
        helm repo update
        helm install node-feature-discovery nfd/node-feature-discovery \
          --namespace node-feature-discovery \
          --create-namespace \
          --set worker.tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set worker.tolerations[0].operator=Exists \
          --set worker.tolerations[0].effect=NoSchedule \
          --timeout 5m || log_warn "NFD install timed out"
        kubectl label namespace node-feature-discovery pod-security.kubernetes.io/enforce=privileged --overwrite
        log_ok "NFD installed"
        
        # Wait for NFD to discover features
        log "Waiting 45s for NFD to discover features..."
        sleep 45
        
        # Install NVIDIA Device Plugin using static manifest (NOT Helm)
        # Helm chart v0.15.0+ has issues on Talos - see issue #12103
        # MIG_STRATEGY=single configured to expose each MIG instance as separate GPU
        log "Installing NVIDIA Device Plugin (static manifest v0.14.5 with MIG_STRATEGY=single)..."
        kubectl apply -f /root/nvidia-device-plugin.yaml
        log_ok "NVIDIA Device Plugin installed"
        
        # Wait for device plugin to be ready
        log "Waiting for device plugin pod..."
        for i in {1..30}; do
          if kubectl -n nvidia-device-plugin get pods -l name=nvidia-device-plugin-ds --no-headers 2>/dev/null | grep -q "Running"; then
            log_ok "Device plugin pod is running"
            break
          fi
          log "  Waiting... ($i/30)"
          sleep 10
        done
        
        # Show device plugin status
        kubectl -n nvidia-device-plugin get pods -o wide || true
        
        # Wait for GPU resources to appear
        log "Waiting for GPU resources to be allocatable..."
        for i in {1..30}; do
          GPU_COUNT=$(kubectl get nodes -o json 2>/dev/null | jq '[.items[].status.allocatable["nvidia.com/gpu"] // "0" | tonumber] | add' || echo "0")
          if [ "$GPU_COUNT" != "null" ] && [ "$GPU_COUNT" != "0" ] && [ -n "$GPU_COUNT" ]; then
            log_ok "Found $GPU_COUNT GPU(s) allocatable!"
            break
          fi
          log "  Waiting for GPUs... ($i/30)"
          sleep 10
        done
        
        # Show device plugin logs if GPU not found
        if [ "$GPU_COUNT" = "0" ] || [ -z "$GPU_COUNT" ]; then
          log_warn "No GPUs found. Checking device plugin logs..."
          kubectl -n nvidia-device-plugin logs -l name=nvidia-device-plugin-ds --tail=50 || true
        fi
        
        # Install GPU Feature Discovery (optional, for advanced labeling)
        log "Installing GPU Feature Discovery..."
        kubectl apply -f /root/gpu-feature-discovery.yaml || log_warn "GFD install failed (optional)"
        
        # MIG Configuration (if enabled)
        if [ "$ENABLE_GPU_MIG" = "true" ]; then
          log "=========================================="
          log "MIG Configuration"
          log "=========================================="
          log_warn "MIG requires manual activation on Talos due to immutable filesystem"
          log "To enable MIG:"
          log "  1. SSH to bastion: make ssh"
          log "  2. Create privileged pod with nvidia-smi"
          log "  3. Run: nvidia-smi -i 0 -mig 1"
          log "  4. Reboot GPU node: talosctl -n <GPU_IP> reboot"
          log "  5. Create MIG instances: nvidia-smi mig -cgi <profiles> -C"
          
          kubectl apply -f /root/mig-config.yaml || true
          log "MIG profiles available: all-disabled, all-1g.10gb, all-2g.20gb, all-3g.40gb, mixed-40-20"
        fi
      fi
      
      # -----------------------------------------------------------------------
      # Final status
      # -----------------------------------------------------------------------
      log "=========================================="
      log "Bootstrap Complete!"
      log "=========================================="
      echo ""
      kubectl get nodes -o wide
      echo ""
      
      if [ $${#GPU_WORKER_IPS[@]} -gt 0 ]; then
        log "GPU Resources:"
        kubectl get nodes -o custom-columns='NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu' || true
        echo ""
        log "NVIDIA Device Plugin Pods:"
        kubectl -n nvidia-device-plugin get pods -o wide || true
        echo ""
        
        # Test GPU access
        log "Testing GPU access with nvidia-smi..."
        kubectl run nvidia-test --rm -it --restart=Never \
          --image=nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04 \
          --overrides='{"spec":{"runtimeClassName":"nvidia","nodeSelector":{"nvidia.com/gpu.present":"true"}}}' \
          --namespace=nvidia-device-plugin \
          --timeout=60s \
          -- nvidia-smi 2>/dev/null || log_warn "nvidia-smi test failed or timed out"
      fi
      
      log "Configs saved to: $CONFIG_DIR"
      log "  kubeconfig:  $CONFIG_DIR/kubeconfig"
      log "  talosconfig: $CONFIG_DIR/talosconfig"
      echo ""
      log "To use:"
      log "  export KUBECONFIG=$CONFIG_DIR/kubeconfig"
      log "  kubectl get nodes"
      log "=========================================="

  # Systemd service
  - path: /etc/systemd/system/talos-bootstrap.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Talos Cluster Bootstrap
      After=network-online.target
      Wants=network-online.target
      
      [Service]
      Type=oneshot
      User=root
      ExecStart=/root/bootstrap.sh
      StandardOutput=journal+console
      StandardError=journal+console
      RemainAfterExit=yes
      
      [Install]
      WantedBy=multi-user.target

runcmd:
  - curl -sL https://talos.dev/install | sh
  - |
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl && mv kubectl /usr/local/bin/
  - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  - systemctl daemon-reload
  - systemctl enable --now talos-bootstrap.service
