# =============================================================================
# NVIDIA GPU Stack for Talos Kubernetes - Standalone Deployment
# =============================================================================
# 
# Ce déploiement gère automatiquement :
# - Détection des nouveaux nœuds GPU (via NFD)
# - Configuration MIG automatique (via MIG Configurator)
# - Exposition des GPU à Kubernetes (via Device Plugin)
# - Persistance MIG après reboot
# - Scaling dynamique (ajout/suppression de nœuds GPU)
#
# Prérequis :
# - Talos avec extensions NVIDIA (nvidia-open-gpu-kernel-modules-production, nvidia-container-toolkit-production)
# - Schematic ID: 22db031c3ec95035687f35472b6f75858473fc7856b40eb44697562db5d0f350
#
# Usage :
#   kubectl apply -f gpu-stack-standalone.yaml
#
# Pour changer le profil MIG :
#   kubectl -n nvidia-gpu-stack edit configmap mig-config
#   # Puis redémarrer les pods MIG configurator sur les nœuds concernés
#
# =============================================================================

---
# =============================================================================
# Namespace
# =============================================================================
apiVersion: v1
kind: Namespace
metadata:
  name: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged

---
# =============================================================================
# RuntimeClass - Requis pour les pods GPU
# =============================================================================
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: runtime
handler: nvidia

---
# =============================================================================
# MIG Configuration - Profils disponibles
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: mig-config
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: mig-config
data:
  # ==========================================================================
  # Profils MIG pour H100 80GB
  # ==========================================================================
  # | Profil    | ID | Mémoire | Instances | Use Case                    |
  # |-----------|----|---------|-----------|-----------------------------|
  # | 1g.10gb   | 19 | 10 GB   | 7         | Inférence légère (Whisper)  |
  # | 2g.20gb   | 14 | 20 GB   | 3         | Inférence moyenne           |
  # | 3g.40gb   | 9  | 40 GB   | 2         | Modèles moyens              |
  # | 4g.40gb   | 5  | 40 GB   | 1         | Demi-GPU                    |
  # | 7g.80gb   | 0  | 80 GB   | 1         | GPU complet                 |
  # | disabled  | -  | 80 GB   | 1         | MIG désactivé               |
  # ==========================================================================
  
  # Configuration active
  MIG_PROFILE: "1g.10gb"
  MIG_PROFILE_ID: "19"
  MIG_INSTANCE_COUNT: "7"
  
  # Activer/désactiver MIG (true/false)
  # Si false, le GPU sera utilisé en mode standard (non-MIG)
  MIG_ENABLED: "true"

---
# =============================================================================
# Node Feature Discovery (NFD) - Détection automatique des GPU
# =============================================================================
# NFD détecte les GPU NVIDIA (PCI vendor 10de) et applique automatiquement
# le label nvidia.com/gpu.present=true sur les nœuds concernés
# =============================================================================

apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfd-master
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfd-worker
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfd-master
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd
rules:
  - apiGroups: [""]
    resources: ["nodes", "nodes/status"]
    verbs: ["get", "list", "watch", "patch", "update"]
  - apiGroups: ["nfd.k8s-sigs.io"]
    resources: ["nodefeatures", "nodefeaturerules"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create", "get", "update"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nfd-master
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nfd-master
subjects:
  - kind: ServiceAccount
    name: nfd-master
    namespace: nvidia-gpu-stack

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfd-worker
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd
rules:
  - apiGroups: ["nfd.k8s-sigs.io"]
    resources: ["nodefeatures"]
    verbs: ["create", "get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nfd-worker
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nfd-worker
subjects:
  - kind: ServiceAccount
    name: nfd-worker
    namespace: nvidia-gpu-stack

---
# NFD Master - Applique les labels sur les nodes
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfd-master
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfd-master
  template:
    metadata:
      labels:
        app: nfd-master
        app.kubernetes.io/name: nvidia-gpu-stack
        app.kubernetes.io/component: nfd-master
    spec:
      serviceAccountName: nfd-master
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              preference:
                matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
      containers:
        - name: nfd-master
          image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
          imagePullPolicy: IfNotPresent
          command:
            - "nfd-master"
          args:
            - "-port=8080"
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 8080
              name: grpc
          resources:
            requests:
              cpu: "10m"
              memory: "32Mi"
            limits:
              cpu: "100m"
              memory: "128Mi"
          securityContext:
            runAsNonRoot: true
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]

---
apiVersion: v1
kind: Service
metadata:
  name: nfd-master
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd-master
spec:
  selector:
    app: nfd-master
  ports:
    - port: 8080
      name: grpc
  type: ClusterIP

---
# NFD Worker - Détecte les features sur chaque node
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nfd-worker
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: nfd-worker
spec:
  selector:
    matchLabels:
      app: nfd-worker
  template:
    metadata:
      labels:
        app: nfd-worker
        app.kubernetes.io/name: nvidia-gpu-stack
        app.kubernetes.io/component: nfd-worker
    spec:
      serviceAccountName: nfd-worker
      dnsPolicy: ClusterFirstWithHostNet
      tolerations:
        - operator: Exists
          effect: NoSchedule
      containers:
        - name: nfd-worker
          image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
          imagePullPolicy: IfNotPresent
          command:
            - "nfd-worker"
          args:
            - "-server=nfd-master.nvidia-gpu-stack.svc.cluster.local:8080"
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
          resources:
            requests:
              cpu: "10m"
              memory: "32Mi"
            limits:
              cpu: "100m"
              memory: "128Mi"
          securityContext:
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
            - name: host-boot
              mountPath: /host-boot
              readOnly: true
            - name: host-os-release
              mountPath: /host-etc/os-release
              readOnly: true
            - name: host-sys
              mountPath: /host-sys
              readOnly: true
            - name: host-lib
              mountPath: /host-lib
              readOnly: true
      volumes:
        - name: host-boot
          hostPath:
            path: /boot
        - name: host-os-release
          hostPath:
            path: /etc/os-release
        - name: host-sys
          hostPath:
            path: /sys
        - name: host-lib
          hostPath:
            path: /lib

---
# =============================================================================
# GPU Labeler - Applique nvidia.com/gpu.present=true basé sur PCI device
# =============================================================================
# Ce job complémente NFD en appliquant le label spécifique NVIDIA
# NFD détecte feature.node.kubernetes.io/pci-10de.present=true (NVIDIA vendor ID)
# Ce controller traduit en nvidia.com/gpu.present=true
# =============================================================================

apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-labeler
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: gpu-labeler

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-labeler
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: gpu-labeler
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch", "patch", "update"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-labeler
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: gpu-labeler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-labeler
subjects:
  - kind: ServiceAccount
    name: gpu-labeler
    namespace: nvidia-gpu-stack

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-labeler
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: gpu-labeler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpu-labeler
  template:
    metadata:
      labels:
        app: gpu-labeler
        app.kubernetes.io/name: nvidia-gpu-stack
        app.kubernetes.io/component: gpu-labeler
    spec:
      serviceAccountName: gpu-labeler
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              preference:
                matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      containers:
        - name: gpu-labeler
          image: bitnami/kubectl:1.31
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "=== GPU Labeler Started ==="
              echo "Watching for nodes with NVIDIA GPUs (PCI vendor 10de)..."
              
              while true; do
                # Trouver les nodes avec GPU NVIDIA (détecté par NFD)
                GPU_NODES=$(kubectl get nodes -l feature.node.kubernetes.io/pci-10de.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null)
                
                for node in $GPU_NODES; do
                  # Vérifier si le label nvidia.com/gpu.present existe déjà
                  CURRENT_LABEL=$(kubectl get node $node -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' 2>/dev/null)
                  
                  if [ "$CURRENT_LABEL" != "true" ]; then
                    echo "[$(date '+%Y-%m-%d %H:%M:%S')] Labeling node $node with nvidia.com/gpu.present=true"
                    kubectl label node $node nvidia.com/gpu.present=true --overwrite
                  fi
                done
                
                # Vérifier aussi les nodes qui ont perdu leur GPU (cleanup)
                LABELED_NODES=$(kubectl get nodes -l nvidia.com/gpu.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null)
                
                for node in $LABELED_NODES; do
                  HAS_GPU=$(kubectl get node $node -o jsonpath='{.metadata.labels.feature\.node\.kubernetes\.io/pci-10de\.present}' 2>/dev/null)
                  
                  if [ "$HAS_GPU" != "true" ]; then
                    echo "[$(date '+%Y-%m-%d %H:%M:%S')] Removing nvidia.com/gpu.present label from $node (no GPU detected)"
                    kubectl label node $node nvidia.com/gpu.present- 2>/dev/null || true
                  fi
                done
                
                sleep 30
              done
          resources:
            requests:
              cpu: "10m"
              memory: "32Mi"
            limits:
              cpu: "50m"
              memory: "64Mi"

---
# =============================================================================
# MIG Configurator - Configure MIG automatiquement sur les nœuds GPU
# =============================================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mig-configurator
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: mig-configurator
spec:
  selector:
    matchLabels:
      app: mig-configurator
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: mig-configurator
        app.kubernetes.io/name: nvidia-gpu-stack
        app.kubernetes.io/component: mig-configurator
    spec:
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoSchedule
      priorityClassName: system-node-critical
      hostPID: true
      
      initContainers:
        - name: configure-mig
          image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          env:
            - name: MIG_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: mig-config
                  key: MIG_ENABLED
            - name: MIG_PROFILE
              valueFrom:
                configMapKeyRef:
                  name: mig-config
                  key: MIG_PROFILE
            - name: MIG_PROFILE_ID
              valueFrom:
                configMapKeyRef:
                  name: mig-config
                  key: MIG_PROFILE_ID
            - name: MIG_INSTANCE_COUNT
              valueFrom:
                configMapKeyRef:
                  name: mig-config
                  key: MIG_INSTANCE_COUNT
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e
              echo "=============================================="
              echo "MIG Configurator - $(date)"
              echo "Node: $NODE_NAME"
              echo "=============================================="
              echo "MIG Enabled: $MIG_ENABLED"
              echo "Profile: $MIG_PROFILE"
              echo "Profile ID: $MIG_PROFILE_ID"
              echo "Instance Count: $MIG_INSTANCE_COUNT"
              echo ""
              
              # Vérifier que nvidia-smi fonctionne
              if ! nvidia-smi &>/dev/null; then
                echo "ERROR: nvidia-smi not available. NVIDIA drivers may not be loaded."
                echo "Waiting for drivers..."
                sleep 30
                exit 1
              fi
              
              # Afficher info GPU
              echo "GPU detected:"
              nvidia-smi -L
              echo ""
              
              # Si MIG désactivé, ne rien faire
              if [ "$MIG_ENABLED" != "true" ]; then
                echo "MIG is disabled in configuration. Skipping MIG setup."
                
                # Vérifier si MIG est actuellement activé
                CURRENT_MIG=$(nvidia-smi -i 0 --query-gpu=mig.mode.current --format=csv,noheader 2>/dev/null || echo "N/A")
                if [ "$CURRENT_MIG" = "Enabled" ]; then
                  echo "WARNING: MIG is currently enabled but configuration says disabled."
                  echo "To disable MIG, run manually: nvidia-smi -mig 0 (then reboot)"
                fi
                exit 0
              fi
              
              # Vérifier le mode MIG actuel
              MIG_MODE=$(nvidia-smi -i 0 --query-gpu=mig.mode.current --format=csv,noheader 2>/dev/null || echo "N/A")
              echo "Current MIG mode: $MIG_MODE"
              
              if [ "$MIG_MODE" = "N/A" ]; then
                echo "ERROR: Could not query MIG mode. GPU may not support MIG."
                exit 0
              fi
              
              if [ "$MIG_MODE" != "Enabled" ]; then
                echo ""
                echo "⚠️  MIG mode is not enabled!"
                echo ""
                echo "To enable MIG mode, run these commands manually:"
                echo "  1. Create a privileged pod on this node"
                echo "  2. Run: nvidia-smi -mig 1"
                echo "  3. Reboot the node: talosctl -n <node-ip> reboot"
                echo ""
                echo "After reboot, MIG instances will be created automatically."
                exit 0
              fi
              
              # MIG est activé, vérifier les instances
              EXISTING_GI=$(nvidia-smi mig -lgi 2>/dev/null | grep -c "MIG" || echo "0")
              echo "Existing GPU instances: $EXISTING_GI"
              
              if [ "$EXISTING_GI" -gt "0" ]; then
                echo ""
                echo "✓ MIG instances already configured:"
                nvidia-smi mig -lgi
                echo ""
                echo "To reconfigure, delete existing instances first:"
                echo "  nvidia-smi mig -dci && nvidia-smi mig -dgi"
                exit 0
              fi
              
              # Créer les instances MIG
              echo ""
              echo "Creating $MIG_INSTANCE_COUNT MIG instances with profile $MIG_PROFILE (ID: $MIG_PROFILE_ID)..."
              
              # Construire la liste des profile IDs
              PROFILE_LIST=""
              for i in $(seq 1 $MIG_INSTANCE_COUNT); do
                if [ -z "$PROFILE_LIST" ]; then
                  PROFILE_LIST="$MIG_PROFILE_ID"
                else
                  PROFILE_LIST="$PROFILE_LIST,$MIG_PROFILE_ID"
                fi
              done
              
              echo "Command: nvidia-smi mig -cgi $PROFILE_LIST -C"
              nvidia-smi mig -cgi $PROFILE_LIST -C
              
              echo ""
              echo "✓ MIG configuration complete!"
              nvidia-smi mig -lgi
              echo ""
              nvidia-smi
              echo ""
              echo "=============================================="
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              cpu: "100m"
              memory: "128Mi"
      
      containers:
        - name: monitor
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c"]
          args:
            - |
              echo "MIG Configurator running on $(hostname)"
              echo "Configuration will be re-applied on pod restart"
              while true; do
                sleep 3600
                echo "[$(date)] MIG Configurator heartbeat"
              done
          resources:
            requests:
              cpu: "5m"
              memory: "16Mi"
            limits:
              cpu: "20m"
              memory: "32Mi"

---
# =============================================================================
# NVIDIA Device Plugin - Expose les GPU à Kubernetes
# =============================================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: device-plugin
spec:
  selector:
    matchLabels:
      app: nvidia-device-plugin
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: nvidia-device-plugin
        app.kubernetes.io/name: nvidia-gpu-stack
        app.kubernetes.io/component: device-plugin
    spec:
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoSchedule
      priorityClassName: system-node-critical
      
      # Attendre que MIG soit configuré
      initContainers:
        - name: wait-for-mig
          image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          env:
            - name: MIG_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: mig-config
                  key: MIG_ENABLED
            - name: MIG_INSTANCE_COUNT
              valueFrom:
                configMapKeyRef:
                  name: mig-config
                  key: MIG_INSTANCE_COUNT
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "Waiting for GPU to be ready..."
              
              # Attendre que nvidia-smi fonctionne
              for i in $(seq 1 60); do
                if nvidia-smi &>/dev/null; then
                  echo "nvidia-smi is available"
                  break
                fi
                echo "Waiting for NVIDIA drivers... ($i/60)"
                sleep 5
              done
              
              # Si MIG est activé, attendre les instances
              if [ "$MIG_ENABLED" = "true" ]; then
                MIG_MODE=$(nvidia-smi -i 0 --query-gpu=mig.mode.current --format=csv,noheader 2>/dev/null || echo "Disabled")
                
                if [ "$MIG_MODE" = "Enabled" ]; then
                  echo "MIG mode is enabled, waiting for instances..."
                  
                  for i in $(seq 1 30); do
                    EXISTING_GI=$(nvidia-smi mig -lgi 2>/dev/null | grep -c "MIG" || echo "0")
                    
                    if [ "$EXISTING_GI" -ge "$MIG_INSTANCE_COUNT" ]; then
                      echo "Found $EXISTING_GI MIG instances (expected: $MIG_INSTANCE_COUNT)"
                      nvidia-smi mig -lgi
                      break
                    fi
                    
                    echo "Waiting for MIG instances... ($EXISTING_GI/$MIG_INSTANCE_COUNT) - attempt $i/30"
                    sleep 10
                  done
                else
                  echo "MIG mode is not enabled. Device plugin will expose full GPU."
                fi
              fi
              
              echo "GPU ready!"
              nvidia-smi -L
          resources:
            limits:
              nvidia.com/gpu: 1
      
      containers:
        - name: nvidia-device-plugin
          image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          env:
            - name: FAIL_ON_INIT_ERROR
              value: "false"
            - name: MIG_STRATEGY
              value: "single"
            - name: PASS_DEVICE_SPECS
              value: "true"
            - name: DEVICE_LIST_STRATEGY
              value: "envvar"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "all"
          volumeMounts:
            - name: device-plugin
              mountPath: /var/lib/kubelet/device-plugins
          resources:
            requests:
              cpu: "10m"
              memory: "32Mi"
            limits:
              cpu: "100m"
              memory: "128Mi"
      
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins

---
# =============================================================================
# GPU Feature Discovery (optionnel mais utile pour les labels détaillés)
# =============================================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-feature-discovery
  namespace: nvidia-gpu-stack
  labels:
    app.kubernetes.io/name: nvidia-gpu-stack
    app.kubernetes.io/component: gpu-feature-discovery
spec:
  selector:
    matchLabels:
      app: gpu-feature-discovery
  template:
    metadata:
      labels:
        app: gpu-feature-discovery
        app.kubernetes.io/name: nvidia-gpu-stack
        app.kubernetes.io/component: gpu-feature-discovery
    spec:
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      serviceAccountName: gpu-labeler
      containers:
        - name: gpu-feature-discovery
          image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
          imagePullPolicy: IfNotPresent
          command: ["gpu-feature-discovery"]
          env:
            - name: GFD_SLEEP_INTERVAL
              value: "60s"
            - name: GFD_MIG_STRATEGY
              value: "single"
            - name: GFD_FAIL_ON_INIT_ERROR
              value: "false"
          securityContext:
            privileged: true
          volumeMounts:
            - name: output-dir
              mountPath: /etc/kubernetes/node-feature-discovery/features.d
            - name: host-sys
              mountPath: /sys
              readOnly: true
          resources:
            requests:
              cpu: "10m"
              memory: "32Mi"
            limits:
              cpu: "100m"
              memory: "128Mi"
      volumes:
        - name: output-dir
          hostPath:
            path: /etc/kubernetes/node-feature-discovery/features.d
            type: DirectoryOrCreate
        - name: host-sys
          hostPath:
            path: /sys
