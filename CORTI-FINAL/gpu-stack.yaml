# =============================================================================
# NVIDIA GPU Stack for Talos - PRODUCTION VERSION
# =============================================================================
# DEPLOYMENT ORDER (CRITICAL for MIG):
#   1. Apply this file (gpu-stack.yaml) - creates namespace, NFD, labeler
#   2. Configure MIG: nvidia-smi -mig 1, reboot, nvidia-smi mig -cgi ...
#   3. Apply device-plugin.yaml - ONLY after MIG instances exist
#
# WHY: Device plugin v0.14.5 crashes if MIG=Enabled but no instances exist
# =============================================================================

---
apiVersion: v1
kind: Namespace
metadata:
  name: nvidia-gpu-stack
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged

---
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mig-config
  namespace: nvidia-gpu-stack
data:
  MIG_ENABLED: "true"
  MIG_PROFILE: "all-1g.10gb"
  MIG_PROFILE_CONFIG: "19,19,19,19,19,19,19"
  MIG_INSTANCE_COUNT: "7"

---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: nodefeatures.nfd.k8s-sigs.io
spec:
  group: nfd.k8s-sigs.io
  names:
    kind: NodeFeature
    listKind: NodeFeatureList
    plural: nodefeatures
    singular: nodefeature
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              x-kubernetes-preserve-unknown-fields: true

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfd-master
  namespace: nvidia-gpu-stack
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfd-worker
  namespace: nvidia-gpu-stack
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator
  namespace: nvidia-gpu-stack

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfd-master
rules:
  - apiGroups: [""]
    resources: ["nodes", "nodes/status"]
    verbs: ["get", "list", "watch", "patch", "update"]
  - apiGroups: ["nfd.k8s-sigs.io"]
    resources: ["nodefeatures"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create", "get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nfd-master
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nfd-master
subjects:
  - kind: ServiceAccount
    name: nfd-master
    namespace: nvidia-gpu-stack

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfd-worker
rules:
  - apiGroups: ["nfd.k8s-sigs.io"]
    resources: ["nodefeatures"]
    verbs: ["create", "get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nfd-worker
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nfd-worker
subjects:
  - kind: ServiceAccount
    name: nfd-worker
    namespace: nvidia-gpu-stack

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch", "patch", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator
subjects:
  - kind: ServiceAccount
    name: gpu-operator
    namespace: nvidia-gpu-stack

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfd-master
  namespace: nvidia-gpu-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfd-master
  template:
    metadata:
      labels:
        app: nfd-master
    spec:
      serviceAccountName: nfd-master
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      containers:
        - name: nfd-master
          image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
          command: ["nfd-master"]
          args: ["-port=8080"]
          ports:
            - containerPort: 8080
          resources:
            requests: { cpu: 10m, memory: 32Mi }
            limits: { cpu: 100m, memory: 128Mi }

---
apiVersion: v1
kind: Service
metadata:
  name: nfd-master
  namespace: nvidia-gpu-stack
spec:
  selector:
    app: nfd-master
  ports:
    - port: 8080

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nfd-worker
  namespace: nvidia-gpu-stack
spec:
  selector:
    matchLabels:
      app: nfd-worker
  template:
    metadata:
      labels:
        app: nfd-worker
    spec:
      serviceAccountName: nfd-worker
      tolerations:
        - operator: Exists
      containers:
        - name: nfd-worker
          image: registry.k8s.io/nfd/node-feature-discovery:v0.16.6
          command: ["nfd-worker"]
          args: ["-server=nfd-master.nvidia-gpu-stack.svc.cluster.local:8080"]
          volumeMounts:
            - name: host-boot
              mountPath: /host-boot
              readOnly: true
            - name: host-sys
              mountPath: /host-sys
              readOnly: true
          resources:
            requests: { cpu: 10m, memory: 32Mi }
            limits: { cpu: 100m, memory: 128Mi }
      volumes:
        - name: host-boot
          hostPath: { path: /boot }
        - name: host-sys
          hostPath: { path: /sys }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-labeler
  namespace: nvidia-gpu-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpu-labeler
  template:
    metadata:
      labels:
        app: gpu-labeler
    spec:
      serviceAccountName: gpu-operator
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      containers:
        - name: labeler
          image: bitnami/kubectl:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              while true; do
                for node in $(kubectl get nodes -l feature.node.kubernetes.io/pci-10de.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
                  kubectl label node $node nvidia.com/gpu.present=true --overwrite 2>/dev/null || true
                done
                sleep 30
              done
          resources:
            requests: { cpu: 10m, memory: 32Mi }
            limits: { cpu: 50m, memory: 64Mi }
